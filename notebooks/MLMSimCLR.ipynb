{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T16:00:09.637984Z",
     "start_time": "2020-10-21T16:00:06.766188Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jsonlines\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import torch_optimizer as optim\n",
    "import random\n",
    "from transformers import AutoModelWithLMHead, AutoTokenizer, AutoModel\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "from importlib import reload\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.float_format', '{:0.3f}'.format)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.options.display.width = 0\n",
    "import warnings\n",
    "import torchvision\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from facebook_hateful_memes_detector.utils.globals import set_global, get_global\n",
    "set_global(\"cache_dir\", \"/home/ahemf/cache/cache2\")\n",
    "# set_global(\"cache_dir\", \"/Users/ahemf/mygit/facebook-hateful-memes/cache\")\n",
    "set_global(\"dataloader_workers\", 32)\n",
    "set_global(\"use_autocast\", True)\n",
    "set_global(\"models_dir\", \"/home/ahemf/cache/\")\n",
    "\n",
    "from facebook_hateful_memes_detector.utils import read_json_lines_into_df, in_notebook, set_device, random_word_mask, dict2sampleList, run_simclr, load_stored_params\n",
    "get_global(\"cache_dir\")\n",
    "from facebook_hateful_memes_detector.models import Fasttext1DCNNModel, MultiImageMultiTextAttentionEarlyFusionModel, LangFeaturesModel, AlbertClassifer\n",
    "from facebook_hateful_memes_detector.preprocessing import TextImageDataset, get_datasets, get_image2torchvision_transforms, TextAugment\n",
    "from facebook_hateful_memes_detector.preprocessing import DefinedRotation, QuadrantCut, ImageAugment, DefinedAffine, HalfSwap, get_transforms_for_bbox_methods\n",
    "from facebook_hateful_memes_detector.preprocessing import get_transforms_for_multiview\n",
    "from facebook_hateful_memes_detector.preprocessing import NegativeSamplingDataset, ImageFolderDataset, ZipDatasets\n",
    "from facebook_hateful_memes_detector.models.MultiModal.VilBertVisualBert import VilBertVisualBertModel\n",
    "from facebook_hateful_memes_detector.models.MultiModal import VilBertVisualBertModelV2, MLMSimCLR, MLMOnlyV2, make_plots\n",
    "from facebook_hateful_memes_detector.training import *\n",
    "import facebook_hateful_memes_detector\n",
    "from facebook_hateful_memes_detector.utils import get_vgg_face_model, get_torchvision_classification_models, init_fc, my_collate, merge_sample_lists\n",
    "reload(facebook_hateful_memes_detector)\n",
    "\n",
    "devices = [\"cuda:0\", \"cuda:0\", \"cuda:1\", \"cuda:2\", \"cuda:3\",]\n",
    "# devices = [\"cpu\"] * 5\n",
    "device = torch.device(devices[0] if torch.cuda.is_available() else \"cpu\")\n",
    "device1 = torch.device(devices[1] if torch.cuda.is_available() else \"cpu\")\n",
    "device2 = torch.device(devices[2] if torch.cuda.is_available() else \"cpu\")\n",
    "device3 = torch.device(devices[3] if torch.cuda.is_available() else \"cpu\")\n",
    "device4 = torch.device(devices[4] if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "set_device(device)\n",
    "print(get_device())\n",
    "\n",
    "scheduler_init_fn = get_cosine_schedule_with_warmup()\n",
    "scheduler_init_fn = get_constant_schedule_with_warmup()\n",
    "set_global(\"gradient_clipping\", 10)\n",
    "# Use mixup in SSL training, Use UDA maybe\n",
    "# os.path.join(get_global(\"models_dir\"),)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T16:00:09.679884Z",
     "start_time": "2020-10-21T16:00:09.639614Z"
    }
   },
   "outputs": [],
   "source": [
    "sep = \" [SEP] \"\n",
    "add_objects_and_caption = True\n",
    "from collections import defaultdict\n",
    "obcp = pd.read_csv(\"/home/ahemf/cache/data/objects_captions.csv\").values\n",
    "object_captions = defaultdict(dict)\n",
    "for d in obcp:\n",
    "    object_captions[d[0]][\"objects\"] = d[1]\n",
    "    object_captions[d[0]][\"caption\"] = d[2]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T16:00:21.944281Z",
     "start_time": "2020-10-21T16:00:09.681540Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "def get_preprocess_text_test_1():\n",
    "    gibberish = {\"punctuation_replace\": 0.25, \"punctuation_strip\": 0.5, \n",
    "                 \"punctuation_insert\": 0.25, \"punctuation_continue\": 0.5}\n",
    "    gibberish = TextAugment([0.5, 0.5], gibberish)\n",
    "    translation = {\"dab\":1.0, \"punctuation_insert\": 0.01}\n",
    "    translation = TextAugment([0.0, 1.0], translation, dab_file=\"/home/ahemf/cache/data/hard_dab.csv\")\n",
    "    def process(text, **kwargs):\n",
    "        text = translation(text, **kwargs)\n",
    "        text = gibberish(text, **kwargs)\n",
    "        return text\n",
    "    return process\n",
    "\n",
    "def get_preprocess_text_test_2():\n",
    "    sentence_level = {\"word_masking\": 1.0}\n",
    "    sentence_level = TextAugment([0.0, 1.0], sentence_level, idf_file=\"/home/ahemf/cache/data/tfidf_terms.csv\")\n",
    "    def process(text, **kwargs):\n",
    "        text = sentence_level(text, **kwargs)\n",
    "        return text\n",
    "    return process\n",
    "\n",
    "def get_preprocess_text_test_3():\n",
    "    char_level = {\"keyboard\": 0.1, \"char_substitute\": 0.4, \"char_insert\": 0.2, \"char_swap\": 0.2, \n",
    "                  \"ocr\": 0.0, \"char_delete\": 0.1}\n",
    "    char_level = TextAugment([0.3, 0.7], char_level)\n",
    "    word_level = {\"split\": 0.2,\n",
    "                 \"stopword_insert\": 0.0, \"word_join\": 0.2, \"punctuation_continue\": 0.5}\n",
    "    word_level = TextAugment([0.3, 0.7], word_level, \n",
    "                             fasttext_file=\"wiki-news-300d-1M-subword.bin\")\n",
    "    gibberish = {\"punctuation_replace\": 0.25, \"punctuation_strip\": 0.5, \"punctuation_insert\": 0.25, \"punctuation_continue\": 0.5}\n",
    "    gibberish = TextAugment([0.3, 0.7], gibberish)\n",
    "    translation = {\"dab\":1.0, \"punctuation_insert\": 0.01}\n",
    "    translation = TextAugment([0.0, 1.0], translation, dab_file=\"/home/ahemf/cache/data/hard_dab.csv\")\n",
    "    \n",
    "    \n",
    "    def process(text, **kwargs):\n",
    "        text = translation(text, **kwargs)\n",
    "        text = word_level(text, **kwargs)\n",
    "        text = char_level(text, **kwargs)\n",
    "        text = gibberish(text, **kwargs)\n",
    "        return text\n",
    "    return process\n",
    "\n",
    "preprocess_text_test_1 = get_preprocess_text_test_1()\n",
    "preprocess_text_test_2 = get_preprocess_text_test_2()\n",
    "preprocess_text_test_3 = get_preprocess_text_test_3()\n",
    "\n",
    "\n",
    "def get_views():\n",
    "    image_views = get_transforms_for_multiview()\n",
    "    def get_view(imv):\n",
    "        imtrans = image_views[imv]\n",
    "        ag_idx = imv + 1\n",
    "        def aug_sample(sample):\n",
    "            if ag_idx == 1:\n",
    "                text = preprocess_text_test_1(sample.original_text, identifier=sample.id)\n",
    "            elif ag_idx == 2:\n",
    "                text = preprocess_text_test_2(sample.original_text, identifier=sample.id)\n",
    "            elif ag_idx == 3:\n",
    "                text = preprocess_text_test_3(sample.original_text, identifier=sample.id)\n",
    "            if ag_idx == 3 and add_objects_and_caption:\n",
    "                text = text + sep + str(object_captions[sample.id][\"caption\"]) + sep + str(object_captions[sample.id][\"objects\"])\n",
    "            sample[\"text_view_%s\" % ag_idx] = text\n",
    "            sample[\"image_view_%s\" % ag_idx] = imtrans(sample.original_image)\n",
    "        return aug_sample\n",
    "    \n",
    "    return [get_view(0), get_view(1), get_view(2)]\n",
    "\n",
    "data = get_datasets(data_dir=\"/home/ahemf/cache/data/\",\n",
    "                    train_text_transform=None,\n",
    "                    train_image_transform=None,\n",
    "                    test_text_transform=None,\n",
    "                    test_image_transform=None,\n",
    "                    train_torchvision_pre_image_transform=None,\n",
    "                    test_torchvision_pre_image_transform=None,\n",
    "                    cache_images=False,\n",
    "                    use_images=True,\n",
    "                    dev=False,\n",
    "                    test_dev=True,\n",
    "                    keep_original_text=True,\n",
    "                    keep_original_image=True,\n",
    "                    keep_processed_image=True,\n",
    "                    keep_torchvision_image=False,\n",
    "                    train_mixup_config=None)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T16:03:33.828120Z",
     "start_time": "2020-10-21T16:00:21.945963Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "def get_preprocess_text():\n",
    "    char_level = {\"keyboard\": 0.1, \"char_substitute\": 0.4, \"char_insert\": 0.2, \"char_swap\": 0.2, \n",
    "                  \"ocr\": 0.0, \"char_delete\": 0.1}\n",
    "    char_level = TextAugment([0.1, 0.9], char_level)\n",
    "    word_level = {\"split\": 0.2,\n",
    "                 \"stopword_insert\": 0.0, \"word_join\": 0.2, \"punctuation_continue\": 0.5}\n",
    "    word_level = TextAugment([0.1, 0.9], word_level, \n",
    "                             fasttext_file=\"wiki-news-300d-1M-subword.bin\")\n",
    "    sentence_level = {\"glove_twitter\": 0.75, \"part_select\": 0.1, }\n",
    "    sentence_level = TextAugment([0.1, 0.9], sentence_level, idf_file=\"/home/ahemf/cache/data/tfidf_terms.csv\"\n",
    "                                )\n",
    "    gibberish = {\"punctuation_replace\": 0.25, \"punctuation_strip\": 0.5, \"punctuation_insert\": 0.25, \"punctuation_continue\": 0.5}\n",
    "    gibberish = TextAugment([0.25, 0.75], gibberish)\n",
    "    translation = {\"dab\":1.0, \"punctuation_insert\": 0.01}\n",
    "    translation = TextAugment([0.0, 1.0], translation, dab_file=\"/home/ahemf/cache/data/hard_dab.csv\")\n",
    "    def process(text, **kwargs):\n",
    "        rng = random.random()\n",
    "        if rng > 0.25 and rng <= 0.45:\n",
    "            text = sentence_level(text, **kwargs)\n",
    "        elif rng > 0.45:\n",
    "            text = translation(text, **kwargs)\n",
    "        if rng > 0.25:\n",
    "            text = word_level(text, **kwargs)\n",
    "            text = char_level(text, **kwargs)\n",
    "            text = gibberish(text, **kwargs)\n",
    "        return text\n",
    "    return process\n",
    "preprocess_text = get_preprocess_text()\n",
    "\n",
    "def get_preprocess_text_test_1():\n",
    "    gibberish = {\"punctuation_replace\": 0.25, \"punctuation_strip\": 0.5, \n",
    "                 \"punctuation_insert\": 0.25, \"punctuation_continue\": 0.5}\n",
    "    gibberish = TextAugment([0.1, 0.9], gibberish)\n",
    "    translation = {\"dab\":1.0, \"punctuation_insert\": 0.01}\n",
    "    translation = TextAugment([0.0, 1.0], translation, dab_file=\"/home/ahemf/cache/data/hard_dab.csv\")\n",
    "    def process(text, **kwargs):\n",
    "        text = translation(text, **kwargs)\n",
    "        text = gibberish(text, **kwargs)\n",
    "        return text\n",
    "    return process\n",
    "\n",
    "def get_preprocess_text_test_2():\n",
    "    char_level = {\"keyboard\": 0.1, \"char_substitute\": 0.4, \"char_insert\": 0.2, \"char_swap\": 0.2, \n",
    "                  \"ocr\": 0.0, \"char_delete\": 0.1}\n",
    "    char_level = TextAugment([0.2, 0.8], char_level)\n",
    "    word_level = {\"split\": 0.2, \"word_join\": 0.2, \"punctuation_continue\": 0.5}\n",
    "    word_level = TextAugment([0.5, 0.5], word_level, \n",
    "                             fasttext_file=\"wiki-news-300d-1M-subword.bin\")\n",
    "    sentence_level = {\"word_masking\": 1.0}\n",
    "    sentence_level = TextAugment([0.9, 0.1], sentence_level, idf_file=\"/home/ahemf/cache/data/tfidf_terms.csv\")\n",
    "    def process(text, **kwargs):\n",
    "        text = word_level(text, **kwargs)\n",
    "        text = char_level(text, **kwargs)\n",
    "        text = sentence_level(text, **kwargs)\n",
    "        return text\n",
    "    return process\n",
    "\n",
    "def get_preprocess_text_test_3():\n",
    "    char_level = {\"keyboard\": 0.1, \"char_substitute\": 0.4, \"char_insert\": 0.2, \"char_swap\": 0.2, \n",
    "                  \"char_delete\": 0.1}\n",
    "    char_level = TextAugment([0.3, 0.7], char_level)\n",
    "    word_level = {\"split\": 0.2, \"word_join\": 0.2, \"punctuation_continue\": 0.5}\n",
    "    word_level = TextAugment([0.3, 0.7], word_level, \n",
    "                             fasttext_file=\"wiki-news-300d-1M-subword.bin\")\n",
    "    gibberish = {\"punctuation_replace\": 0.25, \"punctuation_strip\": 0.5, \"punctuation_insert\": 0.25, \"punctuation_continue\": 0.5}\n",
    "    gibberish = TextAugment([0.3, 0.7], gibberish)\n",
    "    translation = {\"dab\":1.0, \"punctuation_insert\": 0.01}\n",
    "    translation = TextAugment([0.0, 1.0], translation, dab_file=\"/home/ahemf/cache/data/hard_dab.csv\")\n",
    "    \n",
    "    def process(text, **kwargs):\n",
    "        text = translation(text, **kwargs)\n",
    "        text = word_level(text, **kwargs)\n",
    "        text = char_level(text, **kwargs)\n",
    "        text = gibberish(text, **kwargs)\n",
    "        return text\n",
    "    return process\n",
    "\n",
    "preprocess_text_test_1 = get_preprocess_text_test_1()\n",
    "preprocess_text_test_2 = get_preprocess_text_test_2()\n",
    "preprocess_text_test_3 = get_preprocess_text_test_3()\n",
    "\n",
    "def get_views():\n",
    "    image_views = get_transforms_for_multiview()\n",
    "    def get_view(imv):\n",
    "        \n",
    "        imtrans = image_views[imv]\n",
    "        ag_idx = imv + 1\n",
    "        def aug_sample(sample):\n",
    "            if ag_idx == 1:\n",
    "                text = preprocess_text_test_1(sample.original_text, identifier=sample.id)\n",
    "            elif ag_idx == 2:\n",
    "                text = preprocess_text_test_2(sample.original_text, identifier=sample.id)\n",
    "            elif ag_idx == 3:\n",
    "                text = preprocess_text_test_3(sample.original_text, identifier=sample.id)\n",
    "            if ag_idx == 3 and add_objects_and_caption:\n",
    "                text = text + sep + str(object_captions[sample.id][\"caption\"]) + sep + str(object_captions[sample.id][\"objects\"])\n",
    "            sample[\"text_view_%s\" % ag_idx] = text\n",
    "            sample[\"image_view_%s\" % ag_idx] = imtrans(sample.original_image)\n",
    "        return aug_sample\n",
    "    \n",
    "    return [get_view(0), get_view(1), get_view(2)]\n",
    "\n",
    "\n",
    "data = get_datasets(data_dir=\"/home/ahemf/cache/data/\",\n",
    "                    train_text_transform=preprocess_text,\n",
    "                    train_image_transform=get_transforms_for_bbox_methods(),\n",
    "                    test_text_transform=None,\n",
    "                    test_image_transform=None,\n",
    "                    train_torchvision_pre_image_transform=None,\n",
    "                    test_torchvision_pre_image_transform=None,\n",
    "                    cache_images=False,\n",
    "                    use_images=True,\n",
    "                    dev=False,\n",
    "                    test_dev=True,\n",
    "                    keep_original_text=True,\n",
    "                    keep_original_image=True,\n",
    "                    keep_processed_image=True,\n",
    "                    keep_torchvision_image=False,\n",
    "                    train_mixup_config=None)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T16:03:33.842814Z",
     "start_time": "2020-10-21T16:03:33.829545Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12540, 4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"test\"][\"label\"] = -1\n",
    "data['test_unseen'][\"label\"] = -1\n",
    "dev_unseen = data['dev_unseen'].copy()\n",
    "data['dev_unseen'][\"label\"] = -1\n",
    "\n",
    "df = pd.concat((data[\"train\"],\n",
    "                data['dev_unseen'],\n",
    "                data[\"test\"], data['test_unseen']))\n",
    "df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T16:03:33.850704Z",
     "start_time": "2020-10-21T16:03:33.844512Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "dataset = convert_dataframe_to_dataset(df, data[\"metadata\"], True, additional_processors=get_views())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T16:03:34.076168Z",
     "start_time": "2020-10-21T16:03:33.852167Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sample([('id', 42953),\n",
       "        ('text', 'It depneds on her hcaracter, not her oclo ur'),\n",
       "        ('label', 0),\n",
       "        ('sample_weight', 1.0),\n",
       "        ('image',\n",
       "         <PIL.Image.Image image mode=RGB size=265x400 at 0x7F00F7E2E590>),\n",
       "        ('mixup', False),\n",
       "        ('original_image',\n",
       "         <PIL.Image.Image image mode=RGB size=265x400 at 0x7F008FCAA7D0>),\n",
       "        ('original_text', 'its their character not their color that matters'),\n",
       "        ('text_view_1',\n",
       "         \"It'''s not his character keeps his color that counts\"),\n",
       "        ('image_view_1',\n",
       "         <PIL.Image.Image image mode=RGB size=265x400 at 0x7F00FBC4F110>),\n",
       "        ('text_view_2', 'its their character not their color that matters'),\n",
       "        ('image_view_2',\n",
       "         <PIL.Image.Image image mode=RGB size=265x400 at 0x7F00FBC4F890>),\n",
       "        ('text_view_3',\n",
       "         'It depends on her character,. not her colour [SEP] a man in a black shirt and tie looking at the camera [SEP] nose ear man face hair head shirt mouth eye eyes'),\n",
       "        ('image_view_3',\n",
       "         <PIL.Image.Image image mode=RGB size=265x400 at 0x7F00FBC4FD50>)])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T16:03:34.407256Z",
     "start_time": "2020-10-21T16:03:34.077421Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sample([('id', 7826),\n",
       "        ('text',\n",
       "         'any time (when ha&ve to rely upon yowur enemy fora job you \\'\"\"\" re in bad shape!'),\n",
       "        ('label', 0),\n",
       "        ('sample_weight', 1.0),\n",
       "        ('image',\n",
       "         <PIL.Image.Image image mode=RGB size=534x800 at 0x7F00FBBEC650>),\n",
       "        ('mixup', False),\n",
       "        ('original_image',\n",
       "         <PIL.Image.Image image mode=RGB size=534x800 at 0x7F00FBBEC690>),\n",
       "        ('original_text',\n",
       "         \"any time you have to rely upon your enemy for a job you're in bad shape!\"),\n",
       "        ('text_view_1',\n",
       "         'However you have to rely on your enemy for the work that you are in bad shape!'),\n",
       "        ('image_view_1',\n",
       "         <PIL.Image.Image image mode=RGB size=534x800 at 0x7F00FBC4F650>),\n",
       "        ('text_view_2',\n",
       "         \"any tie you ave to rel upon your enemy for a job you ' re in bad shape!\"),\n",
       "        ('image_view_2',\n",
       "         <PIL.Image.Image image mode=RGB size=534x800 at 0x7F00FCFDACD0>),\n",
       "        ('text_view_3',\n",
       "         \"fvery time you have lgft on your *nemy@or a job, that ' s in trouble! [SEP] a man wearing a suit and tie holding a piece of paper [SEP] tie man hand collar mustache nose shirt sleeve hair face\"),\n",
       "        ('image_view_3',\n",
       "         <PIL.Image.Image image mode=RGB size=534x800 at 0x7F01A88E8DD0>)])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1024]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T16:04:24.539999Z",
     "start_time": "2020-10-21T16:03:34.408574Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding option config to projects/hateful_memes/configs/vilbert/from_cc.yaml\n",
      "Overriding option model to vilbert\n",
      "Overriding option datasets to hateful_memes\n",
      "Overriding option run_type to val\n",
      "Overriding option checkpoint.resume_zoo to vilbert.finetuned.hateful_memes.from_cc_original\n",
      "Overriding option evaluation.predict to true\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing ViLBERTBase: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "- This IS expected if you are initializing ViLBERTBase from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing ViLBERTBase from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ViLBERTBase were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.v_embeddings.image_embeddings.weight', 'bert.v_embeddings.image_embeddings.bias', 'bert.v_embeddings.image_location_embeddings.weight', 'bert.v_embeddings.image_location_embeddings.bias', 'bert.v_embeddings.LayerNorm.weight', 'bert.v_embeddings.LayerNorm.bias', 'bert.encoder.v_layer.0.attention.self.query.weight', 'bert.encoder.v_layer.0.attention.self.query.bias', 'bert.encoder.v_layer.0.attention.self.key.weight', 'bert.encoder.v_layer.0.attention.self.key.bias', 'bert.encoder.v_layer.0.attention.self.value.weight', 'bert.encoder.v_layer.0.attention.self.value.bias', 'bert.encoder.v_layer.0.attention.output.dense.weight', 'bert.encoder.v_layer.0.attention.output.dense.bias', 'bert.encoder.v_layer.0.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.0.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.0.intermediate.dense.weight', 'bert.encoder.v_layer.0.intermediate.dense.bias', 'bert.encoder.v_layer.0.output.dense.weight', 'bert.encoder.v_layer.0.output.dense.bias', 'bert.encoder.v_layer.0.output.LayerNorm.weight', 'bert.encoder.v_layer.0.output.LayerNorm.bias', 'bert.encoder.v_layer.1.attention.self.query.weight', 'bert.encoder.v_layer.1.attention.self.query.bias', 'bert.encoder.v_layer.1.attention.self.key.weight', 'bert.encoder.v_layer.1.attention.self.key.bias', 'bert.encoder.v_layer.1.attention.self.value.weight', 'bert.encoder.v_layer.1.attention.self.value.bias', 'bert.encoder.v_layer.1.attention.output.dense.weight', 'bert.encoder.v_layer.1.attention.output.dense.bias', 'bert.encoder.v_layer.1.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.1.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.1.intermediate.dense.weight', 'bert.encoder.v_layer.1.intermediate.dense.bias', 'bert.encoder.v_layer.1.output.dense.weight', 'bert.encoder.v_layer.1.output.dense.bias', 'bert.encoder.v_layer.1.output.LayerNorm.weight', 'bert.encoder.v_layer.1.output.LayerNorm.bias', 'bert.encoder.v_layer.2.attention.self.query.weight', 'bert.encoder.v_layer.2.attention.self.query.bias', 'bert.encoder.v_layer.2.attention.self.key.weight', 'bert.encoder.v_layer.2.attention.self.key.bias', 'bert.encoder.v_layer.2.attention.self.value.weight', 'bert.encoder.v_layer.2.attention.self.value.bias', 'bert.encoder.v_layer.2.attention.output.dense.weight', 'bert.encoder.v_layer.2.attention.output.dense.bias', 'bert.encoder.v_layer.2.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.2.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.2.intermediate.dense.weight', 'bert.encoder.v_layer.2.intermediate.dense.bias', 'bert.encoder.v_layer.2.output.dense.weight', 'bert.encoder.v_layer.2.output.dense.bias', 'bert.encoder.v_layer.2.output.LayerNorm.weight', 'bert.encoder.v_layer.2.output.LayerNorm.bias', 'bert.encoder.v_layer.3.attention.self.query.weight', 'bert.encoder.v_layer.3.attention.self.query.bias', 'bert.encoder.v_layer.3.attention.self.key.weight', 'bert.encoder.v_layer.3.attention.self.key.bias', 'bert.encoder.v_layer.3.attention.self.value.weight', 'bert.encoder.v_layer.3.attention.self.value.bias', 'bert.encoder.v_layer.3.attention.output.dense.weight', 'bert.encoder.v_layer.3.attention.output.dense.bias', 'bert.encoder.v_layer.3.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.3.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.3.intermediate.dense.weight', 'bert.encoder.v_layer.3.intermediate.dense.bias', 'bert.encoder.v_layer.3.output.dense.weight', 'bert.encoder.v_layer.3.output.dense.bias', 'bert.encoder.v_layer.3.output.LayerNorm.weight', 'bert.encoder.v_layer.3.output.LayerNorm.bias', 'bert.encoder.v_layer.4.attention.self.query.weight', 'bert.encoder.v_layer.4.attention.self.query.bias', 'bert.encoder.v_layer.4.attention.self.key.weight', 'bert.encoder.v_layer.4.attention.self.key.bias', 'bert.encoder.v_layer.4.attention.self.value.weight', 'bert.encoder.v_layer.4.attention.self.value.bias', 'bert.encoder.v_layer.4.attention.output.dense.weight', 'bert.encoder.v_layer.4.attention.output.dense.bias', 'bert.encoder.v_layer.4.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.4.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.4.intermediate.dense.weight', 'bert.encoder.v_layer.4.intermediate.dense.bias', 'bert.encoder.v_layer.4.output.dense.weight', 'bert.encoder.v_layer.4.output.dense.bias', 'bert.encoder.v_layer.4.output.LayerNorm.weight', 'bert.encoder.v_layer.4.output.LayerNorm.bias', 'bert.encoder.v_layer.5.attention.self.query.weight', 'bert.encoder.v_layer.5.attention.self.query.bias', 'bert.encoder.v_layer.5.attention.self.key.weight', 'bert.encoder.v_layer.5.attention.self.key.bias', 'bert.encoder.v_layer.5.attention.self.value.weight', 'bert.encoder.v_layer.5.attention.self.value.bias', 'bert.encoder.v_layer.5.attention.output.dense.weight', 'bert.encoder.v_layer.5.attention.output.dense.bias', 'bert.encoder.v_layer.5.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.5.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.5.intermediate.dense.weight', 'bert.encoder.v_layer.5.intermediate.dense.bias', 'bert.encoder.v_layer.5.output.dense.weight', 'bert.encoder.v_layer.5.output.dense.bias', 'bert.encoder.v_layer.5.output.LayerNorm.weight', 'bert.encoder.v_layer.5.output.LayerNorm.bias', 'bert.encoder.c_layer.0.biattention.query1.weight', 'bert.encoder.c_layer.0.biattention.query1.bias', 'bert.encoder.c_layer.0.biattention.key1.weight', 'bert.encoder.c_layer.0.biattention.key1.bias', 'bert.encoder.c_layer.0.biattention.value1.weight', 'bert.encoder.c_layer.0.biattention.value1.bias', 'bert.encoder.c_layer.0.biattention.query2.weight', 'bert.encoder.c_layer.0.biattention.query2.bias', 'bert.encoder.c_layer.0.biattention.key2.weight', 'bert.encoder.c_layer.0.biattention.key2.bias', 'bert.encoder.c_layer.0.biattention.value2.weight', 'bert.encoder.c_layer.0.biattention.value2.bias', 'bert.encoder.c_layer.0.biOutput.dense1.weight', 'bert.encoder.c_layer.0.biOutput.dense1.bias', 'bert.encoder.c_layer.0.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.0.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.0.biOutput.q_dense1.weight', 'bert.encoder.c_layer.0.biOutput.q_dense1.bias', 'bert.encoder.c_layer.0.biOutput.dense2.weight', 'bert.encoder.c_layer.0.biOutput.dense2.bias', 'bert.encoder.c_layer.0.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.0.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.0.biOutput.q_dense2.weight', 'bert.encoder.c_layer.0.biOutput.q_dense2.bias', 'bert.encoder.c_layer.0.v_intermediate.dense.weight', 'bert.encoder.c_layer.0.v_intermediate.dense.bias', 'bert.encoder.c_layer.0.v_output.dense.weight', 'bert.encoder.c_layer.0.v_output.dense.bias', 'bert.encoder.c_layer.0.v_output.LayerNorm.weight', 'bert.encoder.c_layer.0.v_output.LayerNorm.bias', 'bert.encoder.c_layer.0.t_intermediate.dense.weight', 'bert.encoder.c_layer.0.t_intermediate.dense.bias', 'bert.encoder.c_layer.0.t_output.dense.weight', 'bert.encoder.c_layer.0.t_output.dense.bias', 'bert.encoder.c_layer.0.t_output.LayerNorm.weight', 'bert.encoder.c_layer.0.t_output.LayerNorm.bias', 'bert.encoder.c_layer.1.biattention.query1.weight', 'bert.encoder.c_layer.1.biattention.query1.bias', 'bert.encoder.c_layer.1.biattention.key1.weight', 'bert.encoder.c_layer.1.biattention.key1.bias', 'bert.encoder.c_layer.1.biattention.value1.weight', 'bert.encoder.c_layer.1.biattention.value1.bias', 'bert.encoder.c_layer.1.biattention.query2.weight', 'bert.encoder.c_layer.1.biattention.query2.bias', 'bert.encoder.c_layer.1.biattention.key2.weight', 'bert.encoder.c_layer.1.biattention.key2.bias', 'bert.encoder.c_layer.1.biattention.value2.weight', 'bert.encoder.c_layer.1.biattention.value2.bias', 'bert.encoder.c_layer.1.biOutput.dense1.weight', 'bert.encoder.c_layer.1.biOutput.dense1.bias', 'bert.encoder.c_layer.1.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.1.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.1.biOutput.q_dense1.weight', 'bert.encoder.c_layer.1.biOutput.q_dense1.bias', 'bert.encoder.c_layer.1.biOutput.dense2.weight', 'bert.encoder.c_layer.1.biOutput.dense2.bias', 'bert.encoder.c_layer.1.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.1.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.1.biOutput.q_dense2.weight', 'bert.encoder.c_layer.1.biOutput.q_dense2.bias', 'bert.encoder.c_layer.1.v_intermediate.dense.weight', 'bert.encoder.c_layer.1.v_intermediate.dense.bias', 'bert.encoder.c_layer.1.v_output.dense.weight', 'bert.encoder.c_layer.1.v_output.dense.bias', 'bert.encoder.c_layer.1.v_output.LayerNorm.weight', 'bert.encoder.c_layer.1.v_output.LayerNorm.bias', 'bert.encoder.c_layer.1.t_intermediate.dense.weight', 'bert.encoder.c_layer.1.t_intermediate.dense.bias', 'bert.encoder.c_layer.1.t_output.dense.weight', 'bert.encoder.c_layer.1.t_output.dense.bias', 'bert.encoder.c_layer.1.t_output.LayerNorm.weight', 'bert.encoder.c_layer.1.t_output.LayerNorm.bias', 'bert.encoder.c_layer.2.biattention.query1.weight', 'bert.encoder.c_layer.2.biattention.query1.bias', 'bert.encoder.c_layer.2.biattention.key1.weight', 'bert.encoder.c_layer.2.biattention.key1.bias', 'bert.encoder.c_layer.2.biattention.value1.weight', 'bert.encoder.c_layer.2.biattention.value1.bias', 'bert.encoder.c_layer.2.biattention.query2.weight', 'bert.encoder.c_layer.2.biattention.query2.bias', 'bert.encoder.c_layer.2.biattention.key2.weight', 'bert.encoder.c_layer.2.biattention.key2.bias', 'bert.encoder.c_layer.2.biattention.value2.weight', 'bert.encoder.c_layer.2.biattention.value2.bias', 'bert.encoder.c_layer.2.biOutput.dense1.weight', 'bert.encoder.c_layer.2.biOutput.dense1.bias', 'bert.encoder.c_layer.2.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.2.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.2.biOutput.q_dense1.weight', 'bert.encoder.c_layer.2.biOutput.q_dense1.bias', 'bert.encoder.c_layer.2.biOutput.dense2.weight', 'bert.encoder.c_layer.2.biOutput.dense2.bias', 'bert.encoder.c_layer.2.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.2.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.2.biOutput.q_dense2.weight', 'bert.encoder.c_layer.2.biOutput.q_dense2.bias', 'bert.encoder.c_layer.2.v_intermediate.dense.weight', 'bert.encoder.c_layer.2.v_intermediate.dense.bias', 'bert.encoder.c_layer.2.v_output.dense.weight', 'bert.encoder.c_layer.2.v_output.dense.bias', 'bert.encoder.c_layer.2.v_output.LayerNorm.weight', 'bert.encoder.c_layer.2.v_output.LayerNorm.bias', 'bert.encoder.c_layer.2.t_intermediate.dense.weight', 'bert.encoder.c_layer.2.t_intermediate.dense.bias', 'bert.encoder.c_layer.2.t_output.dense.weight', 'bert.encoder.c_layer.2.t_output.dense.bias', 'bert.encoder.c_layer.2.t_output.LayerNorm.weight', 'bert.encoder.c_layer.2.t_output.LayerNorm.bias', 'bert.encoder.c_layer.3.biattention.query1.weight', 'bert.encoder.c_layer.3.biattention.query1.bias', 'bert.encoder.c_layer.3.biattention.key1.weight', 'bert.encoder.c_layer.3.biattention.key1.bias', 'bert.encoder.c_layer.3.biattention.value1.weight', 'bert.encoder.c_layer.3.biattention.value1.bias', 'bert.encoder.c_layer.3.biattention.query2.weight', 'bert.encoder.c_layer.3.biattention.query2.bias', 'bert.encoder.c_layer.3.biattention.key2.weight', 'bert.encoder.c_layer.3.biattention.key2.bias', 'bert.encoder.c_layer.3.biattention.value2.weight', 'bert.encoder.c_layer.3.biattention.value2.bias', 'bert.encoder.c_layer.3.biOutput.dense1.weight', 'bert.encoder.c_layer.3.biOutput.dense1.bias', 'bert.encoder.c_layer.3.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.3.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.3.biOutput.q_dense1.weight', 'bert.encoder.c_layer.3.biOutput.q_dense1.bias', 'bert.encoder.c_layer.3.biOutput.dense2.weight', 'bert.encoder.c_layer.3.biOutput.dense2.bias', 'bert.encoder.c_layer.3.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.3.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.3.biOutput.q_dense2.weight', 'bert.encoder.c_layer.3.biOutput.q_dense2.bias', 'bert.encoder.c_layer.3.v_intermediate.dense.weight', 'bert.encoder.c_layer.3.v_intermediate.dense.bias', 'bert.encoder.c_layer.3.v_output.dense.weight', 'bert.encoder.c_layer.3.v_output.dense.bias', 'bert.encoder.c_layer.3.v_output.LayerNorm.weight', 'bert.encoder.c_layer.3.v_output.LayerNorm.bias', 'bert.encoder.c_layer.3.t_intermediate.dense.weight', 'bert.encoder.c_layer.3.t_intermediate.dense.bias', 'bert.encoder.c_layer.3.t_output.dense.weight', 'bert.encoder.c_layer.3.t_output.dense.bias', 'bert.encoder.c_layer.3.t_output.LayerNorm.weight', 'bert.encoder.c_layer.3.t_output.LayerNorm.bias', 'bert.encoder.c_layer.4.biattention.query1.weight', 'bert.encoder.c_layer.4.biattention.query1.bias', 'bert.encoder.c_layer.4.biattention.key1.weight', 'bert.encoder.c_layer.4.biattention.key1.bias', 'bert.encoder.c_layer.4.biattention.value1.weight', 'bert.encoder.c_layer.4.biattention.value1.bias', 'bert.encoder.c_layer.4.biattention.query2.weight', 'bert.encoder.c_layer.4.biattention.query2.bias', 'bert.encoder.c_layer.4.biattention.key2.weight', 'bert.encoder.c_layer.4.biattention.key2.bias', 'bert.encoder.c_layer.4.biattention.value2.weight', 'bert.encoder.c_layer.4.biattention.value2.bias', 'bert.encoder.c_layer.4.biOutput.dense1.weight', 'bert.encoder.c_layer.4.biOutput.dense1.bias', 'bert.encoder.c_layer.4.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.4.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.4.biOutput.q_dense1.weight', 'bert.encoder.c_layer.4.biOutput.q_dense1.bias', 'bert.encoder.c_layer.4.biOutput.dense2.weight', 'bert.encoder.c_layer.4.biOutput.dense2.bias', 'bert.encoder.c_layer.4.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.4.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.4.biOutput.q_dense2.weight', 'bert.encoder.c_layer.4.biOutput.q_dense2.bias', 'bert.encoder.c_layer.4.v_intermediate.dense.weight', 'bert.encoder.c_layer.4.v_intermediate.dense.bias', 'bert.encoder.c_layer.4.v_output.dense.weight', 'bert.encoder.c_layer.4.v_output.dense.bias', 'bert.encoder.c_layer.4.v_output.LayerNorm.weight', 'bert.encoder.c_layer.4.v_output.LayerNorm.bias', 'bert.encoder.c_layer.4.t_intermediate.dense.weight', 'bert.encoder.c_layer.4.t_intermediate.dense.bias', 'bert.encoder.c_layer.4.t_output.dense.weight', 'bert.encoder.c_layer.4.t_output.dense.bias', 'bert.encoder.c_layer.4.t_output.LayerNorm.weight', 'bert.encoder.c_layer.4.t_output.LayerNorm.bias', 'bert.encoder.c_layer.5.biattention.query1.weight', 'bert.encoder.c_layer.5.biattention.query1.bias', 'bert.encoder.c_layer.5.biattention.key1.weight', 'bert.encoder.c_layer.5.biattention.key1.bias', 'bert.encoder.c_layer.5.biattention.value1.weight', 'bert.encoder.c_layer.5.biattention.value1.bias', 'bert.encoder.c_layer.5.biattention.query2.weight', 'bert.encoder.c_layer.5.biattention.query2.bias', 'bert.encoder.c_layer.5.biattention.key2.weight', 'bert.encoder.c_layer.5.biattention.key2.bias', 'bert.encoder.c_layer.5.biattention.value2.weight', 'bert.encoder.c_layer.5.biattention.value2.bias', 'bert.encoder.c_layer.5.biOutput.dense1.weight', 'bert.encoder.c_layer.5.biOutput.dense1.bias', 'bert.encoder.c_layer.5.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.5.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.5.biOutput.q_dense1.weight', 'bert.encoder.c_layer.5.biOutput.q_dense1.bias', 'bert.encoder.c_layer.5.biOutput.dense2.weight', 'bert.encoder.c_layer.5.biOutput.dense2.bias', 'bert.encoder.c_layer.5.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.5.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.5.biOutput.q_dense2.weight', 'bert.encoder.c_layer.5.biOutput.q_dense2.bias', 'bert.encoder.c_layer.5.v_intermediate.dense.weight', 'bert.encoder.c_layer.5.v_intermediate.dense.bias', 'bert.encoder.c_layer.5.v_output.dense.weight', 'bert.encoder.c_layer.5.v_output.dense.bias', 'bert.encoder.c_layer.5.v_output.LayerNorm.weight', 'bert.encoder.c_layer.5.v_output.LayerNorm.bias', 'bert.encoder.c_layer.5.t_intermediate.dense.weight', 'bert.encoder.c_layer.5.t_intermediate.dense.bias', 'bert.encoder.c_layer.5.t_output.dense.weight', 'bert.encoder.c_layer.5.t_output.dense.bias', 'bert.encoder.c_layer.5.t_output.LayerNorm.weight', 'bert.encoder.c_layer.5.t_output.LayerNorm.bias', 'bert.t_pooler.dense.weight', 'bert.t_pooler.dense.bias', 'bert.v_pooler.dense.weight', 'bert.v_pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding option config to projects/hateful_memes/configs/visual_bert/from_coco.yaml\n",
      "Overriding option model to visual_bert\n",
      "Overriding option datasets to hateful_memes\n",
      "Overriding option run_type to val\n",
      "Overriding option checkpoint.resume_zoo to visual_bert.finetuned.hateful_memes.from_coco\n",
      "Overriding option evaluation.predict to true\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of VisualBERTBase were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.embeddings.token_type_embeddings_visual.weight', 'bert.embeddings.position_embeddings_visual.weight', 'bert.embeddings.projection.weight', 'bert.embeddings.projection.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LXRT encoder with 9 l_layers, 5 x_layers, and 5 r_layers.\n",
      "Overriding option config to projects/hateful_memes/configs/mmbt/with_features.yaml\n",
      "Overriding option model to mmbt\n",
      "Overriding option datasets to hateful_memes\n",
      "Overriding option run_type to val\n",
      "Overriding option checkpoint.resume_zoo to mmbt.hateful_memes.features\n",
      "Overriding option evaluation.predict to true\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config '/local/home/ahemf/mygit/facebook-hateful-memes/facebook_hateful_memes_detector/utils/faster_rcnn_R_101_C4_attr_caffemaxpool.yaml' has no VERSION. Assuming it to be compatible with latest v2.\n",
      "Config '/local/home/ahemf/mygit/facebook-hateful-memes/facebook_hateful_memes_detector/utils/faster_rcnn_R_101_C4_attr_caffemaxpool.yaml' has no VERSION. Assuming it to be compatible with latest v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N tokens Out =  164 Classifier Dims =  768 Matches embedding_dims:  True\n"
     ]
    }
   ],
   "source": [
    "model_params = dict(\n",
    "    model_name={\"lxmert\": dict(dropout=0.01, gaussian_noise=0.01, feature_dropout=0.01),\n",
    "                \"vilbert\": dict(dropout=0.05, gaussian_noise=0.01, feature_dropout=0.02), \n",
    "                \"visual_bert\": dict(dropout=0.05, gaussian_noise=0.01, feature_dropout=0.02),\n",
    "                \"mmbt_region\": dict(dropout=0.05, gaussian_noise=0.01, feature_dropout=0.02)},\n",
    "    num_classes=2,\n",
    "    gaussian_noise=0.01,\n",
    "    dropout=0.1,\n",
    "    word_masking_proba=0.15,\n",
    "    whole_word_masking=True,\n",
    "    featurizer=\"pass\",\n",
    "    final_layer_builder=fb_1d_loss_builder,\n",
    "    internal_dims=768,\n",
    "    classifier_dims=768,\n",
    "    n_tokens_in=128 if add_objects_and_caption else 96,\n",
    "    n_tokens_out=128 if add_objects_and_caption else 96,\n",
    "    n_layers=0,\n",
    "    attention_drop_proba=0.0,\n",
    "    loss=\"focal\",\n",
    "    dice_loss_coef=0.0,\n",
    "    auc_loss_coef=0.0,\n",
    "    bbox_swaps=1,\n",
    "    bbox_copies=1,\n",
    "    bbox_deletes=1,\n",
    "    bbox_gaussian_noise=0.02,\n",
    "    bbox_feature_dropout=0.01,\n",
    "    bbox_dropout=0.05,\n",
    "    view_transforms=get_views(),\n",
    "    view_loss_weight=0.1,\n",
    "    devices=dict(lxmert=device1, vilbert=device4, \n",
    "                 visual_bert=device3, mmbt_region=device2),\n",
    "    finetune=False)\n",
    "\n",
    "model_class = VilBertVisualBertModelV2\n",
    "model = model_class(**model_params)\n",
    "\n",
    "del model.vilbert.model.classifier\n",
    "del model.visual_bert.model.classifier\n",
    "del model.mmbt_region.model.classifier\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T16:04:24.543505Z",
     "start_time": "2020-10-21T16:04:24.541410Z"
    }
   },
   "outputs": [],
   "source": [
    "# torch.save(model.lxmert.state_dict(), os.path.join(get_global(\"models_dir\"),\"lxmert-mlm-init.pth\"))\n",
    "# model.lxmert.load_state_dict(torch.load(os.path.join(get_global(\"models_dir\"),\"lxmert-mlm-init.pth\")))\n",
    "\n",
    "# torch.save(model.mmbt_region.state_dict(), os.path.join(get_global(\"models_dir\"),\"mmbt_region-mlm-init.pth\"))\n",
    "# model.mmbt_region.load_state_dict(torch.load(os.path.join(get_global(\"models_dir\"),\"mmbt_region-mlm-init.pth\")))\n",
    "\n",
    "\n",
    "# torch.save(model.visual_bert.state_dict(), os.path.join(get_global(\"models_dir\"),\"visual_bert-mlm-init.pth\"))\n",
    "# model.visual_bert.load_state_dict(torch.load(os.path.join(get_global(\"models_dir\"),\"visual_bert-mlm-init.pth\")))\n",
    "\n",
    "# torch.save(model.vilbert.state_dict(), os.path.join(get_global(\"models_dir\"),\"vilbert-mlm-init.pth\"))\n",
    "# model.vilbert.load_state_dict(torch.load(os.path.join(get_global(\"models_dir\"),\"vilbert-mlm-init.pth\")))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unimodal MLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T16:04:25.186772Z",
     "start_time": "2020-10-21T16:04:24.544784Z"
    }
   },
   "outputs": [],
   "source": [
    "from facebook_hateful_memes_detector.models.MultiModal.VilBertVisualBertV2 import positive, negative\n",
    "mlm_model = MLMOnlyV2(model, 0.1, {1: negative, 0: positive}, None, mlm_loss_weight=1.0,)\n",
    "# mlm_model = mlm_model.to(get_device())\n",
    "# mlm_model.load_state_dict(torch.load(\"mlm-model-v2.pth\"))\n",
    "# mlm_model.load_state_dict(torch.load(\"mlm-model-v2-view-1.pth\"))\n",
    "# mlm_model = mlm_model.to(get_device())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T16:04:25.190060Z",
     "start_time": "2020-10-21T16:04:25.188168Z"
    }
   },
   "outputs": [],
   "source": [
    "# torch.save(mlm_model.state_dict(), \"mlm-model-v2-view-1.pth\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-10-21T16:00:16.584Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autocast =  True Epochs =  5 Divisor = 1 Examples = 12540 Batch Size =  8\n",
      "Training Samples =  12540 Weighted Sampling =  False Num Batches =  1568 Accumulation steps =  8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9b59c8bc53242798263865acba76228",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4d5f81d6919488488e40f65b34fec44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Batches', max=1568.0, style=ProgressStyle(description_widâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FeatureExtractor : Loaded Model...\n",
      "Modifications for VG in RPN (modeling/proposal_generator/rpn.py):\n",
      "\tUse hidden dim 512 instead fo the same dim as Res4 (1024).\n",
      "\n",
      "Modifications for VG in RoI heads (modeling/roi_heads/fast_rcnn.py))\n",
      "\tEmbedding: 1601 --> 256\tLinear: 2304 --> 512\tLinear: 512 --> 401\n",
      "\n",
      "LXMERTFeatureExtractor : Loaded Model...\n",
      "Exception for:  keyboard | [Original Text]: Do you see him? I don't see him. [Final Text]: Do you see him? I don't see him. \n",
      "[Augs]: ['keyboard'] argument of type 'NoneType' is not iterable\n"
     ]
    }
   ],
   "source": [
    "\n",
    "clean_memory()\n",
    "lr_strategy = {\n",
    "    \"finetune\": True,\n",
    "    \"model\": {\n",
    "        \"vilbert\": {\"finetune\": True,},\n",
    "        \"visual_bert\": {\"finetune\": True,},\n",
    "        \"mmbt_region\": {\"finetune\": True,},\n",
    "        \"lxmert\": {\"finetune\": True,},\n",
    "    },\n",
    "    \"mlm\": {\"finetune\": True},\n",
    "}\n",
    "epochs = 5\n",
    "batch_size = 8 if add_objects_and_caption else 10\n",
    "optimizer_class = torch.optim.AdamW\n",
    "optimizer_params = dict(lr=3e-5, betas=(0.9, 0.98), eps=1e-08, weight_decay=1e-4)\n",
    "# optimizer_params = dict(lr=2e-5, momentum=0.9, weight_decay=1e-3, nesterov=True)\n",
    "\n",
    "_ = group_wise_finetune(mlm_model, lr_strategy)\n",
    "params_conf, _ = group_wise_lr(mlm_model, lr_strategy)\n",
    "optimizer = optimizer_class(params_conf, **optimizer_params)\n",
    "train_losses, learning_rates, _ = train(mlm_model, optimizer, scheduler_init_fn, batch_size, epochs, dataset,\n",
    "                                        model_call_back=None, accumulation_steps=8 if add_objects_and_caption else 6, \n",
    "                                        plot=True,\n",
    "                                        sampling_policy=None, class_weights=None)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T15:55:06.429684Z",
     "start_time": "2020-10-21T15:55:04.994734Z"
    }
   },
   "outputs": [],
   "source": [
    "clean_memory()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T15:55:06.442191Z",
     "start_time": "2020-10-21T15:55:06.432461Z"
    }
   },
   "outputs": [],
   "source": [
    "cache_stats = get_global(\"cache_stats\")\n",
    "cache_stats['get_img_details']\n",
    "cache_stats['get_lxmert_details']\n",
    "\n",
    "train_stats = get_global(\"train_stats\")\n",
    "train_stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-11T18:29:40.513398Z",
     "start_time": "2020-10-11T18:29:40.495558Z"
    }
   },
   "outputs": [],
   "source": [
    "model_parameters = list(filter(lambda p: p.requires_grad, model.lxmert.parameters()))\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "print(\"Trainable Params = %s\" % (params), \"\\n\", model.lxmert)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T05:46:41.193620Z",
     "start_time": "2020-10-21T05:46:20.557766Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "make_plots(model, mlm_model, logy=False, exclude_from_start=5, smoothing=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T07:51:28.432723Z",
     "start_time": "2020-10-20T07:43:45.236941Z"
    }
   },
   "outputs": [],
   "source": [
    "clean_memory()\n",
    "batch_size = 16\n",
    "dev_dataset = convert_dataframe_to_dataset(data['dev'], data[\"metadata\"], False, additional_processors=get_views())\n",
    "validate(mlm_model, batch_size, dev_dataset, display_detail=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T08:00:57.173387Z",
     "start_time": "2020-10-20T07:53:26.296088Z"
    }
   },
   "outputs": [],
   "source": [
    "clean_memory()\n",
    "batch_size = 16\n",
    "dev_dataset = convert_dataframe_to_dataset(data['dev'], data[\"metadata\"], False, additional_processors=get_views())\n",
    "validate(model, batch_size, dev_dataset, display_detail=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T08:09:11.889091Z",
     "start_time": "2020-10-20T08:00:57.175061Z"
    }
   },
   "outputs": [],
   "source": [
    "clean_memory()\n",
    "batch_size = 16\n",
    "dev_dataset = convert_dataframe_to_dataset(data['dev_unseen'], data[\"metadata\"], False, additional_processors=get_views())\n",
    "validate(mlm_model, batch_size, dev_dataset, display_detail=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T08:16:57.260054Z",
     "start_time": "2020-10-20T08:09:11.890868Z"
    }
   },
   "outputs": [],
   "source": [
    "clean_memory()\n",
    "batch_size = 16\n",
    "dev_dataset = convert_dataframe_to_dataset(data['dev_unseen'], data[\"metadata\"], False, additional_processors=get_views())\n",
    "validate(model, batch_size, dev_dataset, display_detail=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T05:53:20.727233Z",
     "start_time": "2020-10-21T05:49:43.862467Z"
    }
   },
   "outputs": [],
   "source": [
    "clean_memory()\n",
    "batch_size = 16\n",
    "preds = predict(mlm_model, data, batch_size, competition_phase=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = predict(model, data, batch_size, competition_phase=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T06:21:48.919432Z",
     "start_time": "2020-10-21T06:21:48.911278Z"
    }
   },
   "outputs": [],
   "source": [
    "preds.to_csv(\"submission_phase_1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-eval\n",
    "- Use back-translation\n",
    "    - Longest 3 backtranslation\n",
    "- Use imgaug\n",
    "    - identity\n",
    "    - HFlip\n",
    "    - GrayScale\n",
    "    - GridDrop\n",
    "    \n",
    "- Use word replace\n",
    "    - Replace top 2 longest word\n",
    "- Use `[masking]`\n",
    "    - 0.15 mask rate `5` times\n",
    "- identity text\n",
    "    \n",
    "(5+3+3)*4 = 44\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
