{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T14:05:40.705092Z",
     "start_time": "2020-06-19T14:05:40.673307Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'facebook_hateful_memes_detector' from '/local/home/ahemf/mygit/facebook-hateful-memes/facebook_hateful_memes_detector/__init__.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<module 'facebook_hateful_memes_detector.preprocessing' from '/local/home/ahemf/mygit/facebook-hateful-memes/facebook_hateful_memes_detector/preprocessing/__init__.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jsonlines\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "from importlib import reload\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.float_format', '{:0.3f}'.format)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.options.display.width = 0\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import facebook_hateful_memes_detector\n",
    "reload(facebook_hateful_memes_detector)\n",
    "reload(facebook_hateful_memes_detector.preprocessing)\n",
    "\n",
    "\n",
    "from facebook_hateful_memes_detector.utils import read_json_lines_into_df, in_notebook\n",
    "from facebook_hateful_memes_detector.models import Fasttext1DCNNModel\n",
    "from facebook_hateful_memes_detector.preprocessing import TextImageDataset, my_collate, get_datasets, TextAugment, clean_text\n",
    "from facebook_hateful_memes_detector.training import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T14:05:42.211083Z",
     "start_time": "2020-06-19T14:05:42.208515Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T14:06:08.771016Z",
     "start_time": "2020-06-19T14:06:08.701846Z"
    }
   },
   "outputs": [],
   "source": [
    "data = get_datasets(data_dir=\"../data/\", train_text_transform=None, train_image_transform=None, \n",
    "                 test_text_transform=None, test_image_transform=None, \n",
    "                 cache_images = True, use_images = False, dev=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T14:06:09.620577Z",
     "start_time": "2020-06-19T14:06:09.607692Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>img</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>42953</td>\n",
       "      <td>../data/img/42953.png</td>\n",
       "      <td>0</td>\n",
       "      <td>its their character not their color that matters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23058</td>\n",
       "      <td>../data/img/23058.png</td>\n",
       "      <td>0</td>\n",
       "      <td>don't be afraid to love again everyone is not ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13894</td>\n",
       "      <td>../data/img/13894.png</td>\n",
       "      <td>0</td>\n",
       "      <td>putting bows on your pet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37408</td>\n",
       "      <td>../data/img/37408.png</td>\n",
       "      <td>0</td>\n",
       "      <td>i love everything and everybody! except for sq...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>82403</td>\n",
       "      <td>../data/img/82403.png</td>\n",
       "      <td>0</td>\n",
       "      <td>everybody loves chocolate chip cookies, even h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>83675</td>\n",
       "      <td>../data/img/83675.png</td>\n",
       "      <td>0</td>\n",
       "      <td>i'm gonna be like phelps one day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>37198</td>\n",
       "      <td>../data/img/37198.png</td>\n",
       "      <td>0</td>\n",
       "      <td>when you're so relaxed you can feel yourself g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>48670</td>\n",
       "      <td>../data/img/48670.png</td>\n",
       "      <td>0</td>\n",
       "      <td>look at this sandwich maker club i found on wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>9863</td>\n",
       "      <td>../data/img/09863.png</td>\n",
       "      <td>0</td>\n",
       "      <td>diverse group of women</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>97320</td>\n",
       "      <td>../data/img/97320.png</td>\n",
       "      <td>0</td>\n",
       "      <td>\"when your dishwasher is broken so you take it...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                    img  label  \\\n",
       "0    42953  ../data/img/42953.png      0   \n",
       "1    23058  ../data/img/23058.png      0   \n",
       "2    13894  ../data/img/13894.png      0   \n",
       "3    37408  ../data/img/37408.png      0   \n",
       "4    82403  ../data/img/82403.png      0   \n",
       "..     ...                    ...    ...   \n",
       "495  83675  ../data/img/83675.png      0   \n",
       "496  37198  ../data/img/37198.png      0   \n",
       "497  48670  ../data/img/48670.png      0   \n",
       "498   9863  ../data/img/09863.png      0   \n",
       "499  97320  ../data/img/97320.png      0   \n",
       "\n",
       "                                                  text  \n",
       "0     its their character not their color that matters  \n",
       "1    don't be afraid to love again everyone is not ...  \n",
       "2                             putting bows on your pet  \n",
       "3    i love everything and everybody! except for sq...  \n",
       "4    everybody loves chocolate chip cookies, even h...  \n",
       "..                                                 ...  \n",
       "495                   i'm gonna be like phelps one day  \n",
       "496  when you're so relaxed you can feel yourself g...  \n",
       "497  look at this sandwich maker club i found on wi...  \n",
       "498                             diverse group of women  \n",
       "499  \"when your dishwasher is broken so you take it...  \n",
       "\n",
       "[9000 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"train\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Augment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T14:06:13.752256Z",
     "start_time": "2020-06-19T14:06:13.749947Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "data_dir=\"../data/\"\n",
    "from functools import partial\n",
    "joiner = partial(os.path.join, data_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T14:06:17.275446Z",
     "start_time": "2020-06-19T14:06:17.234596Z"
    }
   },
   "outputs": [],
   "source": [
    "train = read_json_lines_into_df(joiner('train.jsonl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-04T06:42:24.776334Z",
     "start_time": "2020-06-04T06:42:24.773094Z"
    }
   },
   "outputs": [],
   "source": [
    "train['augmented'] = False\n",
    "train[\"augment_type\"] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-04T06:42:24.787783Z",
     "start_time": "2020-06-04T06:42:24.777554Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>img</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>augmented</th>\n",
       "      <th>augment_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>42953</td>\n",
       "      <td>img/42953.png</td>\n",
       "      <td>0</td>\n",
       "      <td>its their character not their color that matters</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23058</td>\n",
       "      <td>img/23058.png</td>\n",
       "      <td>0</td>\n",
       "      <td>don't be afraid to love again everyone is not ...</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13894</td>\n",
       "      <td>img/13894.png</td>\n",
       "      <td>0</td>\n",
       "      <td>putting bows on your pet</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37408</td>\n",
       "      <td>img/37408.png</td>\n",
       "      <td>0</td>\n",
       "      <td>i love everything and everybody! except for sq...</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>82403</td>\n",
       "      <td>img/82403.png</td>\n",
       "      <td>0</td>\n",
       "      <td>everybody loves chocolate chip cookies, even h...</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id            img  label  \\\n",
       "0  42953  img/42953.png      0   \n",
       "1  23058  img/23058.png      0   \n",
       "2  13894  img/13894.png      0   \n",
       "3  37408  img/37408.png      0   \n",
       "4  82403  img/82403.png      0   \n",
       "\n",
       "                                                text  augmented augment_type  \n",
       "0   its their character not their color that matters      False         None  \n",
       "1  don't be afraid to love again everyone is not ...      False         None  \n",
       "2                           putting bows on your pet      False         None  \n",
       "3  i love everything and everybody! except for sq...      False         None  \n",
       "4  everybody loves chocolate chip cookies, even h...      False         None  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-04T06:42:24.947400Z",
     "start_time": "2020-06-04T06:42:24.788974Z"
    }
   },
   "outputs": [],
   "source": [
    "train_cleaned = train.copy()\n",
    "train_cleaned.text = train_cleaned.text.apply(clean_text)\n",
    "train['augmented'] = True\n",
    "train[\"augment_type\"] = \"clean_text\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-04T06:42:29.100738Z",
     "start_time": "2020-06-04T06:42:29.094452Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     its their character not their color that matters\n",
       "1    don't be afraid to love again everyone is not ...\n",
       "2                             putting bows on your pet\n",
       "3    i love everything and everybody! except for sq...\n",
       "4    everybody loves chocolate chip cookies, even h...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0     its their character not their color that matters\n",
       "1    do not be afraid to love again everyone is not...\n",
       "2                             putting bows on your pet\n",
       "3    i love everything and everybody! except for sq...\n",
       "4    everybody loves chocolate chip cookies, even h...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.text.head()\n",
    "train_cleaned.text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-04T06:42:32.962469Z",
     "start_time": "2020-06-04T06:42:32.957636Z"
    }
   },
   "outputs": [],
   "source": [
    "if in_notebook():\n",
    "    from tqdm.notebook import tqdm, trange\n",
    "else:\n",
    "    from tqdm import tqdm as tqdm, trange\n",
    "    \n",
    "augs = [train, train_cleaned]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_and_append(augs_dict, train, train_cleaned):\n",
    "    augs = []\n",
    "    for k, v in tqdm(augs_dict.items()):\n",
    "        aug_method = v[0]\n",
    "        aug_type_string = v[2]\n",
    "        if \"train\" in v[1]:\n",
    "            t2 = train.copy()\n",
    "            t2[\"augmented\"] = True\n",
    "            t2[\"augment_type\"] = aug_type_string\n",
    "            t2[\"text\"] = [aug_method(t) for t in tqdm(t2.text.values)]\n",
    "            augs.append(t2)\n",
    "        if \"train_cleaned\" in v[1]:\n",
    "            t2 = train_cleaned.copy()\n",
    "            t2[\"augmented\"] = True\n",
    "            t2[\"augment_type\"] = aug_type_string\n",
    "            t2[\"text\"] = [aug_method(t) for t in tqdm(t2.text.values)]\n",
    "    return augs\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_augs = TextAugment(1.0, {\"keyboard\": 0.3, \"ocr\": 0.1, \"char_delete\": 0.1, \"char_insert\": 0.1, \"char_swap\": 0.3,},count=2)\n",
    "sub_augs = TextAugment(1.0, {\"glove_twitter\": 0.25, \"glove_wiki\": 0.25, \"word2vec\": 0.5},count=2)\n",
    "fasttext_augs = TextAugment(1.0, {\"fasttext\": 1.0,},count=1, fasttext_file=\"wiki-news-300d-1M-subword.bin\")\n",
    "sjs_augs = TextAugment(1.0, {\"split\": 0.4, \"stopword_insert\": 0.3, \"word_join\": 0.3,},count=2)\n",
    "syn_augs = TextAugment(1.0, {\"synonym\": 1.0,}, count=1)\n",
    "cutout_augs = TextAugment(1.0, {\"word_cutout\": 1.0,},count=1)\n",
    "sent_augs = TextAugment(1.0, {\"sentence_shuffle\": 0.5 \"text_rotate\": 0.5, \"one_third_cut\": 0.2, \"half_cut\":0.2},count=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DL/Transformers based Aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-06T17:47:13.557240Z",
     "start_time": "2020-06-06T17:47:13.551561Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "import torch\n",
    "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
    "from transformers import pipeline\n",
    "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
    "from transformers import MarianMTModel, MarianTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-06T17:47:13.561916Z",
     "start_time": "2020-06-06T17:47:13.558621Z"
    }
   },
   "outputs": [],
   "source": [
    "ARTICLE = \"\"\" New York (CNN)When Liana Barrientos was 23 years old, she got married in Westchester County, New York.\n",
    "A year later, she got married again in Westchester County, but to a different man and without divorcing her first husband.\n",
    "Only 18 days after that marriage, she got hitched yet again. Then, Barrientos declared \"I do\" five more times, sometimes only within two weeks of each other.\n",
    "In 2010, she married once more, this time in the Bronx. In an application for a marriage license, she stated it was her \"first and only\" marriage.\n",
    "Barrientos, now 39, is facing two criminal counts of \"offering a false instrument for filing in the first degree,\" referring to her false statements on the\n",
    "2010 marriage license application, according to court documents.\n",
    "Prosecutors said the marriages were part of an immigration scam.\n",
    "On Friday, she pleaded not guilty at State Supreme Court in the Bronx, according to her attorney, Christopher Wright, who declined to comment further.\n",
    "After leaving court, Barrientos was arrested and charged with theft of service and criminal trespass for allegedly sneaking into the New York subway through an emergency exit, said Detective\n",
    "Annette Markowski, a police spokeswoman. In total, Barrientos has been married 10 times, with nine of her marriages occurring between 1999 and 2002.\n",
    "All occurred either in Westchester County, Long Island, New Jersey or the Bronx. She is believed to still be married to four men, and at one time, she was married to eight men at once, prosecutors say.\n",
    "Prosecutors said the immigration scam involved some of her husbands, who filed for permanent residence status shortly after the marriages.\n",
    "Any divorces happened only after such filings were approved. It was unclear whether any of the men will be prosecuted.\n",
    "The case was referred to the Bronx District Attorney\\'s Office by Immigration and Customs Enforcement and the Department of Homeland Security\\'s\n",
    "Investigation Division. Seven of the men are from so-called \"red-flagged\" countries, including Egypt, Turkey, Georgia, Pakistan and Mali.\n",
    "Her eighth husband, Rashid Rajput, was deported in 2006 to his native Pakistan after an investigation by the Joint Terrorism Task Force.\n",
    "If convicted, Barrientos faces up to four years in prison.  Her next court appearance is scheduled for May 18.\n",
    "\"\"\"\n",
    "text = 'have you ever studied the history of the jews? did you know that they have always banded together as a tribe, infiltrated governments.'\n",
    "\n",
    "text_long = 'have you ever studied the history of the jews? did you know that they have always banded together as a tribe, infiltrated governments, monopolized the financial systems of nations instigated wars and intentionally created chaos in societies? the jews have mass murdered millions of non- jews over the centuries they have seized control of the media so you will never find out study the history of the jews!'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- summary\n",
    "    - with synonym replacement\n",
    "    - with word2vec based replacement\n",
    "    - with word cutout and stopword insert\n",
    "    - word split and word join\n",
    "- qna with various questions\n",
    "    - with synonym replacement\n",
    "    - with word2vec based replacement\n",
    "    - with word cutout and stopword insert\n",
    "    - word split and word join\n",
    "    - QnA over summary\n",
    "    - QnA over translation\n",
    "- Back translate\n",
    "    - with synonym replacement\n",
    "    - with word2vec based replacement\n",
    "    - with word cutout and stopword insert\n",
    "    - word split and word join\n",
    "    - Over summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QnA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-05T17:57:49.692719Z",
     "start_time": "2020-06-05T17:57:27.221293Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# twmkn9/albert-base-v2-squad2 , twmkn9/distilroberta-base-squad2, huggingface/prunebert-base-uncased-6-finepruned-w-distil-squad\n",
    "# mrm8488/bert-tiny-finetuned-squadv2 , mrm8488/bert-small-finetuned-squadv2, mrm8488/bert-mini-finetuned-squadv2\n",
    "\n",
    "\n",
    "bert_qna_tokenizer = AutoTokenizer.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
    "bert_qna_model = AutoModelForQuestionAnswering.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
    "\n",
    "albert_qna_tokenizer = AutoTokenizer.from_pretrained(\"twmkn9/albert-base-v2-squad2\")\n",
    "albert_qna_model = AutoModelForQuestionAnswering.from_pretrained(\"twmkn9/albert-base-v2-squad2\")\n",
    "\n",
    "tiny_bert_qna_tokenizer = AutoTokenizer.from_pretrained(\"mrm8488/bert-medium-finetuned-squadv2\")\n",
    "tiny_bert_qna_model = AutoModelForQuestionAnswering.from_pretrained(\"mrm8488/bert-medium-finetuned-squadv2\")\n",
    "\n",
    "\n",
    "distilbert_qna_tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-distilled-squad\")\n",
    "distilbert_qna_model = AutoModelForQuestionAnswering.from_pretrained(\"distilbert-base-uncased-distilled-squad\")\n",
    "\n",
    "from transformers import XLNetTokenizer, XLNetForQuestionAnswering, XLNetForQuestionAnsweringSimple\n",
    "import torch\n",
    "\n",
    "XLNet_qna_tokenizer =  XLNetTokenizer.from_pretrained('xlnet-base-cased')\n",
    "XLNet_qna_model = XLNetForQuestionAnsweringSimple.from_pretrained('xlnet-base-cased')\n",
    "\n",
    "\n",
    "qna_models = dict(bert=dict(tokenizer=bert_qna_tokenizer, model=bert_qna_model),\n",
    "                 albert=dict(tokenizer=albert_qna_tokenizer, model=albert_qna_model),\n",
    "                 tiny_bert=dict(tokenizer=tiny_bert_qna_tokenizer, model=tiny_bert_qna_model),\n",
    "                 distilbert=dict(tokenizer=distilbert_qna_tokenizer, model=distilbert_qna_model),\n",
    "                 XLNet=dict(tokenizer=XLNet_qna_tokenizer, model=XLNet_qna_model))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-05T17:58:19.611751Z",
     "start_time": "2020-06-05T17:58:19.601305Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_question_answering(initial_aug=None, qna_model: dict=qna_models[\"bert\"]):\n",
    "    questions = [\"Is this offensive?\", \n",
    "                 \"What part is offensive, bad, misinformed, hurtful?\", \n",
    "                 \"Blatantly misguiding and forming opinion?\", \n",
    "                 \"Only opinions are expressed?\",\n",
    "                \"Targeted towards a particular race, gender, community?\", \n",
    "                 \"Is this hate speech?\",\n",
    "                \"Is there a show of disdain and cynicism?\",\n",
    "                \"Who is responsible and who should be blamed?\",\n",
    "                \"Are they doing the right thing? Should they be corrected?\",\n",
    "                \"We need to stop them!\",\n",
    "                \"Us vs them,\",\n",
    "                 \"What is happening?\",\"Why?\", \"How\", \"When did they?\", \"Thier ways?\"]\n",
    "    from collections import defaultdict\n",
    "    def transform(text):\n",
    "        tokenizer = qna_model[\"tokenizer\"]\n",
    "        model = qna_model[\"model\"]\n",
    "        with torch.no_grad():\n",
    "            text = initial_aug(text) if initial_aug is not None else text\n",
    "            answers = []\n",
    "            batch_inputs = defaultdict(list)\n",
    "            for question in questions:\n",
    "                inputs = tokenizer.encode_plus(question, text, add_special_tokens=True, return_tensors=\"pt\", pad_to_max_length=True, max_length=256)\n",
    "                for k, v in inputs.items():\n",
    "                    batch_inputs[k].append(v)\n",
    "\n",
    "            for k, v in batch_inputs.items():\n",
    "                batch_inputs[k] = torch.cat(v, 0)\n",
    "            answer_start_scores, answer_end_scores = model(**batch_inputs)\n",
    "            answer_start = torch.argmax(answer_start_scores, 1)  # Get the most likely beginning of answer with the argmax of the score\n",
    "            answer_end = torch.argmax(answer_end_scores, 1) + 1 \n",
    "            for i, input_ids in  enumerate(batch_inputs[\"input_ids\"]):\n",
    "                answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start[i]:answer_end[i]]))\n",
    "                answers.append(answer)\n",
    "            answers = list(set([clean_text(a) for a in answers if len(clean_text(a).split())>=2]))\n",
    "            return answers\n",
    "        \n",
    "    return transform\n",
    "\n",
    "qna = get_question_answering(qna_model=qna_models[\"distilbert\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-05T17:58:20.121307Z",
     "start_time": "2020-06-05T17:58:19.810372Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['they have always banded together as a tribe , infiltrated governments',\n",
       " 'infiltrated governments']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qna('have you ever studied the history of the jews? did you know that they have always banded together as a tribe, infiltrated governments.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-06T08:13:28.638640Z",
     "start_time": "2020-06-06T08:11:26.857644Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
    "\n",
    "t5_summarizer_model = AutoModelWithLMHead.from_pretrained(\"t5-base\")\n",
    "t5_summarizer_tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
    "\n",
    "t5_news_tokenizer = AutoTokenizer.from_pretrained(\"mrm8488/t5-base-finetuned-summarize-news\")\n",
    "t5_news_model = AutoModelWithLMHead.from_pretrained(\"mrm8488/t5-base-finetuned-summarize-news\")\n",
    "\n",
    "t5s_summarizer_tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "t5s_summarizer_model = AutoModelWithLMHead.from_pretrained(\"t5-small\")\n",
    "\n",
    "bart_summarizer_tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "bart_summarizer_model = AutoModelWithLMHead.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "\n",
    "ctrl_summarizer_tokenizer = AutoTokenizer.from_pretrained(\"ctrl\")\n",
    "ctrl_summarizer_model = AutoModelWithLMHead.from_pretrained(\"ctrl\")\n",
    "\n",
    "\n",
    "\n",
    "summary_models = dict(ctrl=dict(tokenizer=ctrl_summarizer_tokenizer, model=ctrl_summarizer_model),\n",
    "                    bart=dict(tokenizer=bart_summarizer_tokenizer, model=bart_summarizer_model),\n",
    "                    t5s=dict(tokenizer=t5s_summarizer_tokenizer, model=t5s_summarizer_model),\n",
    "                    t5_news=dict(tokenizer=t5_news_tokenizer, model=t5_news_model),\n",
    "                    t5=dict(tokenizer=t5_summarizer_tokenizer, model=t5_summarizer_model))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-06T08:13:39.358658Z",
     "start_time": "2020-06-06T08:13:28.640797Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['summarize: New York (CNN)When Liana Barrientos was 23 years old, she got married in Westchester County, New York.\\n A year later, she got married again in Westchester County, but to a different man and without divorcing her first husband.\\n Only 18 days after that marriage, she got hitched yet again. Then, Barrientos declared \"I do\" five more times, sometimes only within two weeks of each other.\\n In 2010, she married once more, this time in the Bronx. In an application for a marriage license, she stated it was her \"first and only\" marriage.\\n Barrientos, now 39, is facing two criminal counts of \"offering a false instrument for filing in',\n",
       " '</s><s>Liana Barrientos has been married five times since she was 23 years old. She got married in Westchester County, New York, in',\n",
       " 'Liana Barrientos was 23 years old when she got married in westchester county, new york . only 18 days after that marriage, she',\n",
       " 'Liana Barrientos got married in Westchester County, New York at the age of 23. A year later, she got married again in West',\n",
       " 'Liana Barrientos got married in westchester county, new york, when she was 23 . a year later, she got hitched']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['summarize: have you ever studied the history of the jews? did you know that they have always banded together as a tribe, infiltrated governments.',\n",
       " '</s><s>The jews have always banded together as a tribe, infiltrated governments. Have you ever studied the history of the jews? did you know',\n",
       " 'have you ever studied the history of the jews? did you know that they have always banded together as a tribe, infilt',\n",
       " 'have you ever studied the history of the jews? did you know that they have always banded together as a tribe, infilt',\n",
       " 'have you ever studied the history of the jews? have you ever studied the history of the jews? have you ever studied the history of']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def get_summarizer(models, initial_aug=None, ):\n",
    "    from collections import defaultdict\n",
    "    def transform(text):\n",
    "        summaries = []\n",
    "        with torch.no_grad():\n",
    "            for k, m in models.items():\n",
    "                tokenizer = m[\"tokenizer\"]\n",
    "                model = m[\"model\"]\n",
    "                _ = model.eval()\n",
    "                inputs = tokenizer.encode(\"summarize: \" + text, return_tensors=\"pt\", max_length=128)\n",
    "                outputs = model.generate(inputs, max_length=32, min_length=16, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "                summ = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(outputs[0]))\n",
    "                summaries.append(summ)\n",
    "            return summaries\n",
    "    return transform\n",
    "\n",
    "summary_generator = get_summarizer(summary_models)\n",
    "summary_generator(ARTICLE)\n",
    "summary_generator(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-06T05:56:37.678175Z",
     "start_time": "2020-06-06T05:54:19.072140Z"
    }
   },
   "outputs": [],
   "source": [
    "btdict = dict()\n",
    "\n",
    "model_name = 'Helsinki-NLP/opus-mt-en-tn'\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "btdict[\"en-tn\"] = dict(fwd=(tokenizer, model))\n",
    "\n",
    "model_name = 'Helsinki-NLP/opus-mt-tn-en'\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "btdict[\"en-tn\"][\"inv\"] = (tokenizer, model)\n",
    "\n",
    "\n",
    "model_name = 'Helsinki-NLP/opus-mt-en-ru'\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "btdict[\"en-ru\"] = dict(fwd=(tokenizer, model))\n",
    "\n",
    "model_name = 'Helsinki-NLP/opus-mt-ru-en'\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "btdict[\"en-ru\"][\"inv\"] = (tokenizer, model)\n",
    "\n",
    "\n",
    "model_name = 'Helsinki-NLP/opus-mt-en-de'\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "btdict[\"en-de\"] = dict(fwd=(tokenizer, model))\n",
    "\n",
    "model_name = 'Helsinki-NLP/opus-mt-de-en'\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "btdict[\"en-de\"][\"inv\"] = (tokenizer, model)\n",
    "\n",
    "\n",
    "model_name = 'Helsinki-NLP/opus-mt-en-CELTIC'\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "btdict[\"en-CELTIC\"] = dict(fwd=(tokenizer, model))\n",
    "\n",
    "model_name = 'sshleifer/opus-mt-CELTIC-en'\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "btdict[\"en-CELTIC\"][\"inv\"] = (tokenizer, model)\n",
    "\n",
    "\n",
    "model_name = 'Helsinki-NLP/opus-mt-en-ROMANCE'\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "btdict[\"en-ROMANCE\"] = dict(fwd=(tokenizer, model))\n",
    "\n",
    "model_name = 'Helsinki-NLP/opus-mt-ROMANCE-en'\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "btdict[\"en-ROMANCE\"][\"inv\"] = (tokenizer, model)\n",
    "\n",
    "model_name = 'Helsinki-NLP/opus-mt-en-chk'\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "btdict[\"en-chk\"] = dict(fwd=(tokenizer, model))\n",
    "\n",
    "model_name = 'Helsinki-NLP/opus-mt-chk-en'\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "btdict[\"en-chk\"][\"inv\"] = (tokenizer, model)\n",
    "\n",
    "model_name = 'Helsinki-NLP/opus-mt-en-jap'\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "btdict[\"en-jap\"] = dict(fwd=(tokenizer, model))\n",
    "\n",
    "model_name = 'Helsinki-NLP/opus-mt-jap-en'\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "btdict[\"en-jap\"][\"inv\"] = (tokenizer, model)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-06T06:23:22.339566Z",
     "start_time": "2020-06-06T06:23:22.332449Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "def get_back_translation_model(btdict):\n",
    "    def back_translate(text):\n",
    "        texts = [text]\n",
    "        tl = len(text.split())\n",
    "        answers = []\n",
    "        for k, v in btdict.items():\n",
    "            fwd_tokenizer, fwd_model = v[\"fwd\"]\n",
    "            inv_tokenizer, inv_model = v[\"inv\"]\n",
    "            lang_codes = fwd_tokenizer.supported_language_codes\n",
    "            if \"ROMANCE\" in k:\n",
    "                lang_codes = ['>>fr<<', '>>es<<', '>>it<<', '>>pt<<', '>>ro<<', '>>ca<<', '>>gl<<', '>>la<<', '>>wa<<', '>>fur<<', '>>oc<<', '>>sc<<', '>>an<<', '>>frp<<',]\n",
    "            if len(lang_codes) > 0:\n",
    "                texts = [t for text in texts for t in [lang+\" \"+text for lang in lang_codes]]\n",
    "            translated = fwd_model.generate(**fwd_tokenizer.prepare_translation_batch(texts))\n",
    "            \n",
    "            translated = inv_model.generate(**inv_tokenizer.prepare_translation_batch([fwd_tokenizer.decode(t, skip_special_tokens=True) for t in translated]))\n",
    "            tgt_text = [inv_tokenizer.decode(t, skip_special_tokens=True) for t in translated]\n",
    "            answers.append(tgt_text)\n",
    "        answers = [a for ans in answers for a in ans]\n",
    "        ans_lens = [len(clean_text(a).split()) for a in answers]\n",
    "        answers = list(set([a for l,a in zip(ans_lens,answers) if l>=2 and l>=tl/2]))\n",
    "        return answers\n",
    "    return back_translate\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-06T06:23:22.331187Z",
     "start_time": "2020-06-06T06:22:42.955472Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['They had a story to the history of the jewels? they wanted to make bonds with them all as a tribe of wildlife, an intellectual ruler.',\n",
       " 'Have you ever studied the history of the Jews? Did you know that they have always joined as a tribe, infiltrated governments.',\n",
       " 'you have never studied the history of jews? did you know that they have always banded together as a tribe, infiltrated governments.',\n",
       " 'Have you ever studied the history of the Jews? You knew that they have always united as tribes, infiltrated governments.',\n",
       " 'Have you ever studied the history of the jewels? do you know that they have always banded together as a tribe, undergraduate governments.',\n",
       " 'When you first learned about history, you knew that they had helped one another as a group, the deceived nations.',\n",
       " 'Studying the history of journalism? you know that they were always banded together as a tribe, infiltrated governments.',\n",
       " 'Did you ever direct the history of the jews? they knew that they never bonded together as a tribe, instrumental governments.',\n",
       " \"Is it the story of the journal's history? that they wanted bands to evolve through, infiltrated governments.\",\n",
       " 'Have you ever studied the history of people who speak different languages?',\n",
       " 'How long wilt thou make an end of thy work? they know that they are gathered together as judges in the judgment seat.']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "back_translate = get_back_translation_model(btdict)\n",
    "back_translate(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-06T06:31:48.076942Z",
     "start_time": "2020-06-06T06:25:54.748970Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43.8 s ± 3.82 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit back_translate(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-06T08:11:03.991382Z",
     "start_time": "2020-06-06T08:07:07.271044Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/ahemf/.cache/torch/hub/pytorch_fairseq_master\n",
      "Using cache found in /home/ahemf/.cache/torch/hub/pytorch_fairseq_master\n",
      "Using cache found in /home/ahemf/.cache/torch/hub/pytorch_fairseq_master\n",
      "Using cache found in /home/ahemf/.cache/torch/hub/pytorch_fairseq_master\n",
      "Using cache found in /home/ahemf/.cache/torch/hub/pytorch_fairseq_master\n",
      "Using cache found in /home/ahemf/.cache/torch/hub/pytorch_fairseq_master\n",
      "Using cache found in /home/ahemf/.cache/torch/hub/pytorch_fairseq_master\n",
      "Using cache found in /home/ahemf/.cache/torch/hub/pytorch_fairseq_master\n"
     ]
    }
   ],
   "source": [
    "models_dict= dict()\n",
    "import torch\n",
    "en2de = torch.hub.load('pytorch/fairseq', 'transformer.wmt19.en-de.single_model', tokenizer='moses', bpe='fastbpe')\n",
    "de2en = torch.hub.load('pytorch/fairseq', 'transformer.wmt19.de-en.single_model', tokenizer='moses', bpe='fastbpe')\n",
    "en2de_cp4 = torch.hub.load('pytorch/fairseq', 'transformer.wmt19.en-de',\n",
    "                       checkpoint_file='model1.pt:model2.pt:model3.pt:model4.pt',\n",
    "                       tokenizer='moses', bpe='fastbpe')\n",
    "\n",
    "de2en_cp4 = torch.hub.load('pytorch/fairseq', 'transformer.wmt19.de-en',\n",
    "                       checkpoint_file='model1.pt:model2.pt:model3.pt:model4.pt',\n",
    "                       tokenizer='moses', bpe='fastbpe')\n",
    "en2de_wmt16 = torch.hub.load('pytorch/fairseq', 'transformer.wmt16.en-de', tokenizer='moses', bpe='fastbpe')\n",
    "en2de_wmt17 = torch.hub.load('pytorch/fairseq', 'conv.wmt17.en-de', tokenizer='moses', bpe='fastbpe')\n",
    "en2ru = torch.hub.load('pytorch/fairseq', 'transformer.wmt19.en-ru.single_model', tokenizer='moses', bpe='fastbpe')\n",
    "ru2en = torch.hub.load('pytorch/fairseq', 'transformer.wmt19.ru-en.single_model', tokenizer='moses', bpe='fastbpe')\n",
    "\n",
    "models_dict[\"en-de\"]=dict(fwd=[en2de, en2de_cp4, en2de_wmt16, en2de_wmt17], inv=[de2en, de2en_cp4])\n",
    "models_dict[\"en-ru\"] = dict(fwd=[en2ru], inv=[ru2en])\n",
    "\n",
    "\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-06T08:11:26.856414Z",
     "start_time": "2020-06-06T08:11:04.457345Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['have you ever studied the history of the jews? did you know that they have always gagged as tribes, infiltrated governments.',\n",
       " 'have you ever studied the history of the jews? did you know that as a tribe, they have always gagged infiltrated governments.',\n",
       " 'Have you ever studied the history of jewels? Did you know that they have always united as three-sided, infiltrated governments.',\n",
       " 'Have you ever studied the history of the Jews? Did you know that they have always united as a tribe and infiltrated governments?',\n",
       " 'Have you ever studied the history of the Jews? Did you know that they always came together as a tribe, infiltrated by governments.',\n",
       " 'Have you ever studied the history of jewels? Did you know that they have always united as tripartite, infiltrated governments.']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_pytorch_backtranslate(mdict):\n",
    "    def py_translate(text):\n",
    "        tl = len(text.split())\n",
    "        answers = []\n",
    "        for k, v in mdict.items():\n",
    "            fwd = v[\"fwd\"]\n",
    "            inv = v[\"inv\"]\n",
    "            for fm in fwd:\n",
    "                intermediate = fm.translate(text)\n",
    "                for bm in inv:\n",
    "                    res = bm.translate(intermediate)\n",
    "                    answers.append(res)\n",
    "        ans_lens = [len(clean_text(a).split()) for a in answers]\n",
    "        answers = list(set([a for l,a in zip(ans_lens,answers) if l>=2 and l>=tl/2]))\n",
    "        return answers\n",
    "    return py_translate\n",
    "\n",
    "py_translate = get_pytorch_backtranslate(models_dict)\n",
    "py_translate(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-07T11:45:41.671622Z",
     "start_time": "2020-06-07T11:42:44.407310Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd159f87d86e4fde9aa4a40f3bb7a1bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=230.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "020ab5c675cc47ffa5ce3683eed752e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=230.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87fdeb3f655a47d686c4fece9db5ea4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=230.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da0fb0b7e570452aa8af4c74228242fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=230.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5acc5efdbf147b6a7714a8716b5a3d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=230.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "gen_model_names = [\"gpt2\", \"gpt2-medium\", \"gpt2-large\", \"gpt2-xl\", \"microsoft/DialoGPT-large\"]\n",
    "\n",
    "gen_models = [pipeline(\"text-generation\")] + [pipeline(\"text-generation\", model=m, tokenizer=m) for m in gen_model_names]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-07T16:21:48.595623Z",
     "start_time": "2020-06-07T16:17:48.958272Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-07 16:17:48 WARNING: Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "2020-06-07 16:17:51 WARNING: Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "2020-06-07 16:17:55 WARNING: Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "2020-06-07 16:17:57 WARNING: Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "2020-06-07 16:18:02 WARNING: Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "2020-06-07 16:18:05 WARNING: Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "2020-06-07 16:18:07 WARNING: Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "2020-06-07 16:18:10 WARNING: Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "2020-06-07 16:18:13 WARNING: Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "2020-06-07 16:18:15 WARNING: Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "2020-06-07 16:18:18 WARNING: Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "2020-06-07 16:18:20 WARNING: Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "2020-06-07 16:18:21 WARNING: Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "2020-06-07 16:18:22 WARNING: Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "2020-06-07 16:18:23 WARNING: Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "2020-06-07 16:18:24 WARNING: Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "2020-06-07 16:18:25 WARNING: Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "2020-06-07 16:18:27 WARNING: Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "2020-06-07 16:18:28 WARNING: Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "2020-06-07 16:18:29 WARNING: Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "2020-06-07 16:18:29 WARNING: Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "2020-06-07 16:18:30 WARNING: Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "2020-06-07 16:18:31 WARNING: Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "2020-06-07 16:18:31 WARNING: Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "2020-06-07 16:18:32 WARNING: Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "2020-06-07 16:18:33 WARNING: Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "2020-06-07 16:18:35 WARNING: Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "2020-06-07 16:18:36 WARNING: Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "2020-06-07 16:18:38 WARNING: Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "2020-06-07 16:18:39 WARNING: Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "2020-06-07 16:18:41 WARNING: Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "2020-06-07 16:18:42 WARNING: Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "2020-06-07 16:18:43 WARNING: Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "2020-06-07 16:18:45 WARNING: Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "2020-06-07 16:18:46 WARNING: Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "2020-06-07 16:18:47 WARNING: Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "2020-06-07 16:18:51 WARNING: Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "2020-06-07 16:18:55 WARNING: Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "2020-06-07 16:19:03 WARNING: Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "2020-06-07 16:19:06 WARNING: Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "2020-06-07 16:19:10 WARNING: Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "2020-06-07 16:19:13 WARNING: Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "2020-06-07 16:19:17 WARNING: Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "2020-06-07 16:19:19 WARNING: Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "2020-06-07 16:19:22 WARNING: Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "2020-06-07 16:19:24 WARNING: Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "2020-06-07 16:19:26 WARNING: Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "2020-06-07 16:19:32 WARNING: Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "2020-06-07 16:19:37 WARNING: Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "2020-06-07 16:19:47 WARNING: Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "2020-06-07 16:20:00 WARNING: Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "2020-06-07 16:20:13 WARNING: Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "2020-06-07 16:20:17 WARNING: Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "2020-06-07 16:20:23 WARNING: Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "2020-06-07 16:20:29 WARNING: Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "2020-06-07 16:20:33 WARNING: Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "2020-06-07 16:20:37 WARNING: Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "2020-06-07 16:20:40 WARNING: Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "2020-06-07 16:20:43 WARNING: Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "2020-06-07 16:20:46 WARNING: Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "2020-06-07 16:20:50 WARNING: Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "2020-06-07 16:20:51 WARNING: Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "2020-06-07 16:20:52 WARNING: Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "2020-06-07 16:20:56 WARNING: Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "2020-06-07 16:21:03 WARNING: Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "2020-06-07 16:21:08 WARNING: Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "2020-06-07 16:21:16 WARNING: Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "2020-06-07 16:21:20 WARNING: Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "2020-06-07 16:21:27 WARNING: Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "2020-06-07 16:21:31 WARNING: Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "2020-06-07 16:21:39 WARNING: Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "2020-06-07 16:21:44 WARNING: Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['0: have you ever studied the history of the jews? did you know that they have always banded together as a tribe, infiltrated governments. Did you ever know that jews know the unspoken regulations of the land that our country',\n",
       " \"0: have you ever studied the history of the jews? did you all somehow think the jews were good, if not the fair? did you know there were hundreds of 'ducks' of coloured parts, only four-fif\",\n",
       " \"0: know that they have always banded together as a tribe, infiltrated governments. But by now, it's all too familiar. At the very least, they have known each other before.\\n\\nThis summer, more and more people\",\n",
       " '0: have you ever studied the history of the jews? did you know that they have always banded together as a tribe, infiltrated governments. now they have joined together as a community?\"\\n\\n\\nAnd, \"but I know all',\n",
       " \"0: have you ever studied the history of the jews? did you see a lot of them?\\n\\n\\nLurker: Never. People did know about these people. They don't talk about them now. They're on the\",\n",
       " '0: know that they have always banded together as a tribe, infiltrated governments. [7] It has been stated that she and The Prophet of Christ had a relationship known as the Confession of Faith. This was not the case for',\n",
       " '0: have you ever studied the history of the jews? did you know that they have always banded together as a tribe, infiltrated governments.. Our opinion is that no matter how ancient there is like the Greek, Antigua,',\n",
       " '0: have you ever studied the history of the jews? did you know that they have always banded together as a tribe, infiltrated governments.. You know, these are great people and how do you come up with an argument about how',\n",
       " '0: have you ever studied the history of the jews? did you know that they have always banded together as a tribe, infiltrated governments.. They are well known for their black magic, and both Adolf Hitler and Benjamin Brown, leader',\n",
       " '0: have you ever studied the history of the jews? did you know that they have always banded together as a tribe, infiltrated governments.. I think they are due for an excommunication as soon as we give them our vote?',\n",
       " '0: have you ever studied the history of the jews? did you know that they have always banded together as a tribe, infiltrated governments.. Their poeple have done it wrong!! The best way to tell if the jews',\n",
       " '0: have you ever studied the history of the jews? did you know that they have always banded together as a tribe, infiltrated governments.. In the past those guys have always done interesting things like overthrowing US President George Washington,\\n----------------------------------------------------------------------------------------------------\\n',\n",
       " '1: have you ever studied the history of the jews? did you know that they have always banded together as a tribe, infiltrated governments. They become so afraid of every good man of great strength and prosperity that they say \"Get',\n",
       " '1: have you ever studied the history of the jews? did you have a lover (or worse) both being held by J.I.P. Lee in 1888? Or at least are you pretty sure them?) On display are',\n",
       " \"1: know that they have always banded together as a tribe, infiltrated governments. One of her best friends is captured by the CIA. She learns of the war's true origins, and sends a team to be careful with their communications.\",\n",
       " '1: have you ever studied the history of the jews? did you know that they have always banded together as a tribe, infiltrated governments. they have been trained, financed, and trained as they are now? And how does this',\n",
       " \"1: have you ever studied the history of the jews? did you play with them till a little while ago? do you know a lot about them? what about jews who are killed in our country's history or do they die\",\n",
       " \"1: know that they have always banded together as a tribe, infiltrated governments. However, when the last remnants of their tribe were defeated by the North's last ally, they turned their attention to the South and tried to make their way\",\n",
       " '1: have you ever studied the history of the jews? did you know that they have always banded together as a tribe, infiltrated governments.. Our opinion is it? if you wondered why, we checked it.\\n\\nand you',\n",
       " '1: have you ever studied the history of the jews? did you know that they have always banded together as a tribe, infiltrated governments.. You know those mighty empires which have now so far made the two kingdoms independent completely but by',\n",
       " '1: have you ever studied the history of the jews? did you know that they have always banded together as a tribe, infiltrated governments.. They are well known for their allegiance to Nazis, when they were on their crusade against the',\n",
       " '1: have you ever studied the history of the jews? did you know that they have always banded together as a tribe, infiltrated governments.. I think they are as much interested in fighting their own tribes as they are in modern reality',\n",
       " '1: have you ever studied the history of the jews? did you know that they have always banded together as a tribe, infiltrated governments.. Their poeple have done nearly all of the damage to us.. Their wars have gone',\n",
       " '1: have you ever studied the history of the jews? did you know that they have always banded together as a tribe, infiltrated governments.. In the past those guys took over Japan and America and cut down the currency.. Some of\\n----------------------------------------------------------------------------------------------------\\n',\n",
       " \"2: have you ever studied the history of the jews? did you know that they have always banded together as a tribe, infiltrated governments. they are always hidden away in the Illuminati? You can't even take class 6 notes in\",\n",
       " '2: have you ever studied the history of the jews? did you eat up every last piece of it?\\' \"\\'Yes, a little.\\' \"\\'Perhaps,\\' said he, \\'you may be able to tell what rank you hold.\\' \"\\'',\n",
       " '2: know that they have always banded together as a tribe, infiltrated governments. Let them know just what a despicable group they are.\" Trump used the power of the presidency to curtail the conservative movement and embolden the resistance movement, and',\n",
       " '2: have you ever studied the history of the jews? did you know that they have always banded together as a tribe, infiltrated governments. just like the Romans did? as soon as they are faced with the obvious choice. they',\n",
       " \"2: have you ever studied the history of the jews? did you know that they're not only a jew, but the most feared people in the world??? You know those people who worship the jews at their temple? Well\",\n",
       " '2: know that they have always banded together as a tribe, infiltrated governments. … Well, all I can do is tell you that one of them had the ability to manipulate others by having them be led into doing his bidding and then',\n",
       " '2: have you ever studied the history of the jews? did you know that they have always banded together as a tribe, infiltrated governments.. Our opinion is, that they deserve to be forgiven for what they have done.\\n\\n\\n',\n",
       " '2: have you ever studied the history of the jews? did you know that they have always banded together as a tribe, infiltrated governments.. You know.. why do you think they havent...\\n\\nYeah, and the government',\n",
       " '2: have you ever studied the history of the jews? did you know that they have always banded together as a tribe, infiltrated governments.. They are well known for bullying and instilling distrust in everyone. They make sure that every',\n",
       " '2: have you ever studied the history of the jews? did you know that they have always banded together as a tribe, infiltrated governments.. I think they are behind 100% of the violence around the world. this mass genocide is',\n",
       " '2: have you ever studied the history of the jews? did you know that they have always banded together as a tribe, infiltrated governments.. Their poeple have done huge damage in the world through espionage and wanton tyranny.',\n",
       " '2: have you ever studied the history of the jews? did you know that they have always banded together as a tribe, infiltrated governments.. In the past those guys have been an important part of the Roman State and they ruled through\\n----------------------------------------------------------------------------------------------------\\n',\n",
       " '3: have you ever studied the history of the jews? did you know that they have always banded together as a tribe, infiltrated governments. many times. but they have always been forbidden to. as for this: believe me,',\n",
       " '3: have you ever studied the history of the jews? did you see his \\'official\\' views? how is it possible to say that he was an \"atheist\" while calling for an atheist revolution.\\n\\nPossible responses',\n",
       " '3: know that they have always banded together as a tribe, infiltrated governments. and are in constant surveillance of people, taking photos and listening to conversations. All they ever hear, for good reason, is exactly what they want it to',\n",
       " \"3: have you ever studied the history of the jews? did you know that they have always banded together as a tribe, infiltrated governments. the only reason they don't control the banks is because jews were there to put them\",\n",
       " '3: have you ever studied the history of the jews? did you have any good experience? You know there is no need to feel uncomfortable when you come here. do you wish to ask me anything else? Good night\"',\n",
       " '3: know that they have always banded together as a tribe, infiltrated governments.\\n\\nWhy they don\\'t just blow us all up and run off. Because, in the words of one of our very good, trusted friends: \"',\n",
       " '3: have you ever studied the history of the jews? did you know that they have always banded together as a tribe, infiltrated governments.. Our opinion is 100% in favor of jews.. and that is why Israel has a',\n",
       " '3: have you ever studied the history of the jews? did you know that they have always banded together as a tribe, infiltrated governments.. You know.. The history of their people is infinite.. have you watched the great depression?',\n",
       " '3: have you ever studied the history of the jews? did you know that they have always banded together as a tribe, infiltrated governments.. They are well known for their uncanny ability to gather, sell and then infiltrate cultures.. Have',\n",
       " '3: have you ever studied the history of the jews? did you know that they have always banded together as a tribe, infiltrated governments.. I think they had to when they found out the USA was the most powerful empire on the',\n",
       " '3: have you ever studied the history of the jews? did you know that they have always banded together as a tribe, infiltrated governments.. Their poeple have done the deed together... what, people, ARE YOU READY',\n",
       " '3: have you ever studied the history of the jews? did you know that they have always banded together as a tribe, infiltrated governments.. In the past those guys were the vikings.. still are... they think they own\\n----------------------------------------------------------------------------------------------------\\n',\n",
       " '4: have you ever studied the history of the jews? did you know that they have always banded together as a tribe, infiltrated governments. they are terrorist in origin.. while others may call them jews.... yes.. there has',\n",
       " '4: have you ever studied the history of the jews? did you know jews are the dindus of the world you are missing the point i am nothing but a nigger im the most dangerous man in the world your on',\n",
       " \"4: know that they have always banded together as a tribe, infiltrated governments. They are like child's play to the real terrorists, like little ants to be made fun of, like ants and bumble bees and the like. They\",\n",
       " '4: have you ever studied the history of the jews? did you know that they have always banded together as a tribe, infiltrated governments. I am not saying, do they not know how to use this power of persuasion, manipulation',\n",
       " \"4: have you ever studied the history of the jews? did you ever wonder how we got here, how the jews ended up like this, where we're going, why, etc. etc.? so many myths, so little\",\n",
       " '4: know that they have always banded together as a tribe, infiltrated governments. That you know.\\n\\n\"Even if the United States was not a nation, it is more than a country. I love this nation. I love',\n",
       " '4: have you ever studied the history of the jews? did you know that they have always banded together as a tribe, infiltrated governments.. Our opinion is that these people are aligned with humanity more than they are with race.\" –',\n",
       " '4: have you ever studied the history of the jews? did you know that they have always banded together as a tribe, infiltrated governments.. You know what I mean... THEY HAVE MASSIVE ARMY WANTS.. That',\n",
       " '4: have you ever studied the history of the jews? did you know that they have always banded together as a tribe, infiltrated governments.. They are well known for their treason and plotting against religions. This crime of genocide is how',\n",
       " '4: have you ever studied the history of the jews? did you know that they have always banded together as a tribe, infiltrated governments.. I think they have \"found some new nerve\" if you could call it that, with',\n",
       " '4: have you ever studied the history of the jews? did you know that they have always banded together as a tribe, infiltrated governments.. Their poeple have done terrible things in the past..and now they are doing the',\n",
       " '4: have you ever studied the history of the jews? did you know that they have always banded together as a tribe, infiltrated governments.. In the past those guys were called catholics, they were part of a much older group\\n----------------------------------------------------------------------------------------------------\\n',\n",
       " '5: have you ever studied the history of the jews? did you know that they have always banded together as a tribe, infiltrated governments.lol',\n",
       " '5: have you ever studied the history of the jews? did you do the research yourself? historians work long hours.',\n",
       " \"5: know that they have always banded together as a tribe, infiltrated governments.It'll take some serious dedication to try to go through all the channels channels.\",\n",
       " '5: have you ever studied the history of the jews? did you know that they have always banded together as a tribe, infiltrated governments.Its why Jews are all over the world today',\n",
       " '5: have you ever studied the history of the jews? did you ever understand that they were a historical figure who was literally murdered.',\n",
       " '5: know that they have always banded together as a tribe, infiltrated governments.It is the part that changed my opinion on this whole thing.',\n",
       " '5: have you ever studied the history of the jews? did you know that they have always banded together as a tribe, infiltrated governments.. Our opinion is enough to say to save in peace of mind',\n",
       " '5: have you ever studied the history of the jews? did you know that they have always banded together as a tribe, infiltrated governments.. You know, and when they successfully attack and stop...',\n",
       " \"5: have you ever studied the history of the jews? did you know that they have always banded together as a tribe, infiltrated governments.. They are well known for they're cult mentality if you'll believe it!\",\n",
       " '5: have you ever studied the history of the jews? did you know that they have always banded together as a tribe, infiltrated governments.. I think they had a claim to the desert. nomnomnomnomnomnomnomnom',\n",
       " '5: have you ever studied the history of the jews? did you know that they have always banded together as a tribe, infiltrated governments.. Their poeple have done this for thousands and thousands of years.',\n",
       " '5: have you ever studied the history of the jews? did you know that they have always banded together as a tribe, infiltrated governments.. In the past those guys are still around.\\n----------------------------------------------------------------------------------------------------\\n']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_gen_models(gen_models):\n",
    "    def generate(text, nlp):\n",
    "        spaced_tokens = text.split()\n",
    "        n_tokens = len(spaced_tokens)\n",
    "        nlp1 = lambda t: nlp(t,max_length=n_tokens*2, clean_up_tokenization_spaces=True, do_sample=True, top_p=0.97, top_k=500)[0]['generated_text']\n",
    "        nlp2 = lambda t: nlp(t,max_length=n_tokens*2, clean_up_tokenization_spaces=True, do_sample=True, top_p=0.95, top_k=100)[0]['generated_text']\n",
    "        if n_tokens <= 3:\n",
    "            return [text]\n",
    "        t1 = nlp1(text)\n",
    "        t2 = nlp1(\" \".join(spaced_tokens[:int(n_tokens/2)]))\n",
    "        t3 = nlp1(\" \".join(spaced_tokens[int(n_tokens/2):]))\n",
    "        t4 = nlp2(text)\n",
    "        t5 = nlp2(\" \".join(spaced_tokens[:int(n_tokens/2)]))\n",
    "        t6 = nlp2(\" \".join(spaced_tokens[int(n_tokens/2):]))\n",
    "        \n",
    "        t7 = nlp1(text + \". Our opinion is\")\n",
    "        t8 = nlp1(text + \". You know\")\n",
    "        t9 = nlp1(text + \". They are well known for\")\n",
    "        t10 =  nlp1(text + \". I think they\")\n",
    "        t11 = nlp1(text + \". Their poeple have done\")\n",
    "        t12 = nlp1(text + \". In the past those guys\")\n",
    "        \n",
    "        return [t1, t2, t3, t4, t5, t6, t7, t8, t9, t10, t11, t12]\n",
    "    \n",
    "    def transform(text):\n",
    "        answers=[]\n",
    "        for i, nlp in enumerate(gen_models):\n",
    "            generated = generate(text, nlp)\n",
    "            generated = [str(i) + \": \"+ g for g in generated]\n",
    "            generated[-1] = generated[-1] + \"\\n\" + \"-\"*100 + \"\\n\"\n",
    "            answers.extend(generated)\n",
    "        return answers\n",
    "    return transform\n",
    "\n",
    "text_gen = get_gen_models(gen_models)\n",
    "text_gen(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-06T11:04:57.964510Z",
     "start_time": "2020-06-06T11:03:25.272922Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "488ed2aa944e4b60aa56adf3bc259457",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=230.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2fc3aaa754146868fea0455a81552f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=230.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f6e5204ca7c499eb11c14d8086097e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=230.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "982d8cee4c9641fc847480db5b812ee3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=230.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "fill_mask_model_names = [\"distilroberta-base\", \"bert-base-uncased\", \n",
    "                    \"albert-base-v2\", \"Hate-speech-CNERG/dehatebert-mono-english\", \n",
    "                    \"novinsh/xlm-roberta-large-toxicomments-12k\",\"allenai/dsp_roberta_base_dapt_reviews_tapt_amazon_helpfulness_115K\",\n",
    "                   \"allenai/dsp_roberta_base_dapt_reviews_tapt_imdb_70000\", \"allenai/reviews_roberta_base\", \"ssun32/bert_twitter_turkle\",\n",
    "                   \"huggingface/distilbert-base-uncased-finetuned-mnli\", \"activebus/BERT_Review\",\n",
    "                        \"distilbert-base-uncased-distilled-squad\"]\n",
    "\n",
    "# \"mrm8488/t5-base-finetuned-span-sentiment-extraction\",\n",
    "# \"mrm8488/distilroberta-base-finetuned-sentiment\",\n",
    "\n",
    "fill_mask_models = [pipeline(\"fill-mask\")]+[pipeline(\"fill-mask\", model=m, tokenizer=m) for m in fill_mask_model_names]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-07T12:20:01.879802Z",
     "start_time": "2020-06-07T12:20:01.628753Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fill_mask_models' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-814644aef1b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m \u001b[0mfill_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_fill_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfill_mask_models\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0mfill_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'fill_mask_models' is not defined"
     ]
    }
   ],
   "source": [
    "import random\n",
    "def get_fill_mask(fill_mask_models):\n",
    "    def fill_mask_ntimes(text, nlp, n=2):\n",
    "        mask = nlp.tokenizer.mask_token\n",
    "        for _ in range(n):\n",
    "            spaced_tokens = clean_text(text).split()\n",
    "            n_tokens = len(spaced_tokens)\n",
    "            token_lengths = list(map(len, spaced_tokens))\n",
    "            token_lengths = np.array([0 if t<=2 else t for t in token_lengths])\n",
    "            token_probas = token_lengths / token_lengths.sum()\n",
    "            mask_pos = np.random.choice(list(range(n_tokens)), 1, replace=False, p=token_probas)[0]\n",
    "            spaced_tokens[mask_pos] = mask\n",
    "            masked_text = \" \".join(spaced_tokens)\n",
    "            replies = nlp(masked_text)\n",
    "            text = [r['sequence'] for r in replies][0]\n",
    "        return text\n",
    "            \n",
    "        \n",
    "    \n",
    "    def fill_mask(text):\n",
    "        spaced_tokens = clean_text(text).split()\n",
    "        n_tokens = len(spaced_tokens)\n",
    "        token_lengths = list(map(len, spaced_tokens))\n",
    "        token_lengths = np.array([0 if t<=2 else t for t in token_lengths])\n",
    "        token_probas = token_lengths / token_lengths.sum()\n",
    "        answers = []\n",
    "        for i, nlp in enumerate(fill_mask_models):\n",
    "            mask = nlp.tokenizer.mask_token\n",
    "            mask_pos = np.random.choice(list(range(n_tokens)), 3, replace=False, p=token_probas)\n",
    "            for p in mask_pos:\n",
    "                tks = list(spaced_tokens)\n",
    "                tks[p] = mask\n",
    "                masked_text = \" \".join(tks)\n",
    "                replies = nlp(masked_text)\n",
    "                replies = [r['sequence'] for r in replies][:2]\n",
    "                answers.extend(replies)\n",
    "        answers = list(set([clean_text(a) for a in answers]))\n",
    "        return answers\n",
    "    \n",
    "    def fill_mask(text):\n",
    "        answers = []\n",
    "        for i, nlp in enumerate(fill_mask_models):\n",
    "            t = fill_mask_ntimes(text, nlp, n=2)\n",
    "            answers.append(t)\n",
    "        answers = list(set([clean_text(a) for a in answers]))\n",
    "        return answers\n",
    "    \n",
    "    return fill_mask\n",
    "\n",
    "    \n",
    "\n",
    "fill_mask = get_fill_mask(fill_mask_models)\n",
    "fill_mask(text)\n",
    "\n",
    "        \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Image and Back to Word via OCR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
